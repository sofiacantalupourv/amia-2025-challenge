{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf6e7b0-6875-4ea7-8327-effcea26feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics version: 8.3.130\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d86ea08-07a7-48df-9d46-a3a48b210ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: GPU-73280671-c9c4-70a5-0e54-7f98b2a44f29\n",
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "GPU 0: NVIDIA RTX 4000 Ada Generation\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"Not set\"))\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Check GPU access\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c5481a-6baf-4020-b0b3-1991f5effde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/data/mhedas/common/challenge_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a0f546-3a71-4fdf-9230-81ce665dd394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45925 annotations\n",
      "Number of unique images: 8573\n",
      "\n",
      "Class distribution:\n",
      "Class 0: 5481 annotations\n",
      "Class 1: 255 annotations\n",
      "Class 2: 851 annotations\n",
      "Class 3: 4046 annotations\n",
      "Class 4: 519 annotations\n",
      "Class 5: 904 annotations\n",
      "Class 6: 1097 annotations\n",
      "Class 7: 2188 annotations\n",
      "Class 8: 2324 annotations\n",
      "Class 9: 1945 annotations\n",
      "Class 10: 2190 annotations\n",
      "Class 11: 4308 annotations\n",
      "Class 12: 195 annotations\n",
      "Class 13: 4097 annotations\n",
      "Class 14: 15525 annotations\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load and preprocess the data\n",
    "train_df = pd.read_csv(os.path.join(root_path, \"train.csv\"))\n",
    "img_dims_df = pd.read_csv(os.path.join(root_path, \"img_size.csv\"))\n",
    "train_df = train_df.merge(img_dims_df, on='image_id', how='left')\n",
    "\n",
    "print(f\"Loaded {len(train_df)} annotations\")\n",
    "print(f\"Number of unique images: {train_df['image_id'].nunique()}\")\n",
    "\n",
    "# Print class distribution\n",
    "class_counts = train_df['class_id'].value_counts().sort_index()\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343dc557-78b6-44f0-84dc-4b9cdababd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dimensions for 15000 images\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a mapping of original image dimensions from img_size.csv\n",
    "def load_image_dimensions(csv_path):\n",
    "    \"\"\"Load image dimensions from CSV file\"\"\"\n",
    "    img_dimensions = {}\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Store dimensions as (height, width)\n",
    "        img_dimensions[row['image_id']] = (int(row['dim0']), int(row['dim1']))\n",
    "    \n",
    "    return img_dimensions\n",
    "\n",
    "original_dimensions = load_image_dimensions(os.path.join(root_path, \"img_size.csv\"))\n",
    "print(f\"Loaded dimensions for {len(original_dimensions)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328275a7-3f95-4fe3-a250-ff117f0805d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNG_HEIGHT, PNG_WIDTH = 1024, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0c5adf-51dc-424a-b301-36dbc20cbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Function to scale bounding box coordinates\n",
    "def scale_bbox(bbox, original_dims, new_dims):\n",
    "    \"\"\"\n",
    "    Scale bounding box coordinates from original dimensions to new dimensions\n",
    "    \n",
    "    Args:\n",
    "        bbox (list/array): Bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "        original_dims (tuple): Original image dimensions (height, width)\n",
    "        new_dims (tuple): New image dimensions (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        list: Scaled bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = original_dims\n",
    "    new_height, new_width = new_dims\n",
    "    \n",
    "    # Scale factors\n",
    "    width_scale = new_width / orig_width\n",
    "    height_scale = new_height / orig_height\n",
    "    \n",
    "    # Scale coordinates\n",
    "    x_min = bbox[0] * width_scale\n",
    "    y_min = bbox[1] * height_scale\n",
    "    x_max = bbox[2] * width_scale\n",
    "    y_max = bbox[3] * height_scale\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f4e957-f741-4e32-bd37-5403ab915741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create directories for YOLOv8 dataset format (initial train/val split)\n",
    "# These directories will be used for HP tuning and Gauge Training\n",
    "initial_dataset_dir_name = 'yolov8_dataset_initial_split'\n",
    "os.makedirs(f'{initial_dataset_dir_name}/images/train', exist_ok=True)\n",
    "os.makedirs(f'{initial_dataset_dir_name}/images/val', exist_ok=True)\n",
    "os.makedirs(f'{initial_dataset_dir_name}/labels/train', exist_ok=True)\n",
    "os.makedirs(f'{initial_dataset_dir_name}/labels/val', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19edbe9f-e29c-4356-8e4a-ab408ad3bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing stratified split to maintain class distribution...\n",
      "Class 0: 2365 unique images\n",
      "Class 1: 167 unique images\n",
      "Class 2: 385 unique images\n",
      "Class 3: 1746 unique images\n",
      "Class 4: 325 unique images\n",
      "Class 5: 341 unique images\n",
      "Class 6: 532 unique images\n",
      "Class 7: 1132 unique images\n",
      "Class 8: 705 unique images\n",
      "Class 9: 965 unique images\n",
      "Class 10: 909 unique images\n",
      "Class 11: 1689 unique images\n",
      "Class 12: 79 unique images\n",
      "Class 13: 1388 unique images\n",
      "Class 14: 5175 unique images\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Split data into training and validation sets with stratification\n",
    "print(\"\\nPerforming stratified split to maintain class distribution...\")\n",
    "\n",
    "# Get unique image IDs for each class\n",
    "class_image_ids = {}\n",
    "for class_id in range(15):  # 0-14 classes\n",
    "    class_image_ids[class_id] = set(train_df[train_df['class_id'] == class_id]['image_id'].unique())\n",
    "\n",
    "# Count how many images have each class\n",
    "for class_id, img_ids in class_image_ids.items():\n",
    "    print(f\"Class {class_id}: {len(img_ids)} unique images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74cf011-50c1-43b8-b2f3-3ae3db60ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training images: 6857\n",
      "Initial validation images: 1716\n"
     ]
    }
   ],
   "source": [
    "all_image_ids = train_df['image_id'].unique()\n",
    "rare_classes = [0, 2, 5, 6, 12, 13]\n",
    "images_with_rare_classes = set()\n",
    "for class_id in rare_classes:\n",
    "    images_with_rare_classes.update(class_image_ids.get(class_id, set()))\n",
    "\n",
    "rare_images = list(images_with_rare_classes)\n",
    "common_images = list(set(all_image_ids) - images_with_rare_classes)\n",
    "\n",
    "# Split rare and common images separately with the same ratio\n",
    "rare_train, rare_val = train_test_split(rare_images, test_size=0.2, random_state=111)\n",
    "common_train, common_val = train_test_split(common_images, test_size=0.2, random_state=111)\n",
    "\n",
    "# Combine the splits\n",
    "train_images = rare_train + common_train\n",
    "val_images = rare_val + common_val\n",
    "\n",
    "print(f\"Initial training images: {len(train_images)}\")\n",
    "print(f\"Initial validation images: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90d21aee-d995-44df-9f49-6f0df2056a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Training set class distribution (annotations):\n",
      "Class 0: 4342 annotations (11.8%)\n",
      "Class 1: 210 annotations (0.6%)\n",
      "Class 2: 672 annotations (1.8%)\n",
      "Class 3: 3258 annotations (8.9%)\n",
      "Class 4: 421 annotations (1.1%)\n",
      "Class 5: 742 annotations (2.0%)\n",
      "Class 6: 870 annotations (2.4%)\n",
      "Class 7: 1748 annotations (4.8%)\n",
      "Class 8: 1800 annotations (4.9%)\n",
      "Class 9: 1566 annotations (4.3%)\n",
      "Class 10: 1766 annotations (4.8%)\n",
      "Class 11: 3420 annotations (9.3%)\n",
      "Class 12: 172 annotations (0.5%)\n",
      "Class 13: 3310 annotations (9.0%)\n",
      "Class 14: 12396 annotations (33.8%)\n",
      "\n",
      "Initial Validation set class distribution (annotations):\n",
      "Class 0: 1139 annotations (12.3%)\n",
      "Class 1: 45 annotations (0.5%)\n",
      "Class 2: 179 annotations (1.9%)\n",
      "Class 3: 788 annotations (8.5%)\n",
      "Class 4: 98 annotations (1.1%)\n",
      "Class 5: 162 annotations (1.8%)\n",
      "Class 6: 227 annotations (2.5%)\n",
      "Class 7: 440 annotations (4.8%)\n",
      "Class 8: 524 annotations (5.7%)\n",
      "Class 9: 379 annotations (4.1%)\n",
      "Class 10: 424 annotations (4.6%)\n",
      "Class 11: 888 annotations (9.6%)\n",
      "Class 12: 23 annotations (0.2%)\n",
      "Class 13: 787 annotations (8.5%)\n",
      "Class 14: 3129 annotations (33.9%)\n"
     ]
    }
   ],
   "source": [
    "train_df_subset = train_df[train_df['image_id'].isin(train_images)]\n",
    "val_df_subset = train_df[train_df['image_id'].isin(val_images)]\n",
    "\n",
    "train_class_counts_annotations = train_df_subset['class_id'].value_counts().sort_index()\n",
    "val_class_counts_annotations = val_df_subset['class_id'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nInitial Training set class distribution (annotations):\")\n",
    "for class_id, count in train_class_counts_annotations.items():\n",
    "    print(f\"Class {class_id}: {count} annotations ({count/sum(train_class_counts_annotations)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nInitial Validation set class distribution (annotations):\")\n",
    "for class_id, count in val_class_counts_annotations.items():\n",
    "    print(f\"Class {class_id}: {count} annotations ({count/sum(val_class_counts_annotations)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ad8875-2605-4b8f-9bd6-697039d019c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting initial training annotations to YOLOv8 format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [03:24<00:00, 33.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4132 empty label files for 'No finding' images.\n",
      "Processed 24297 annotations for actual ailment classes (0-13).\n",
      "Counts of each class_id written to label files (class 14 indicates empty files):\n",
      "Class 0: 4342 entries\n",
      "Class 1: 210 entries\n",
      "Class 2: 672 entries\n",
      "Class 3: 3258 entries\n",
      "Class 4: 421 entries\n",
      "Class 5: 742 entries\n",
      "Class 6: 870 entries\n",
      "Class 7: 1748 entries\n",
      "Class 8: 1800 entries\n",
      "Class 9: 1566 entries\n",
      "Class 10: 1766 entries\n",
      "Class 11: 3420 entries\n",
      "Class 12: 172 entries\n",
      "Class 13: 3310 entries\n",
      "Class 14: 4132 entries\n",
      "\n",
      "Converting initial validation annotations to YOLOv8 format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1716/1716 [00:35<00:00, 47.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1043 empty label files for 'No finding' images.\n",
      "Processed 6103 annotations for actual ailment classes (0-13).\n",
      "Counts of each class_id written to label files (class 14 indicates empty files):\n",
      "Class 0: 1139 entries\n",
      "Class 1: 45 entries\n",
      "Class 2: 179 entries\n",
      "Class 3: 788 entries\n",
      "Class 4: 98 entries\n",
      "Class 5: 162 entries\n",
      "Class 6: 227 entries\n",
      "Class 7: 440 entries\n",
      "Class 8: 524 entries\n",
      "Class 9: 379 entries\n",
      "Class 10: 424 entries\n",
      "Class 11: 888 entries\n",
      "Class 12: 23 entries\n",
      "Class 13: 787 entries\n",
      "Class 14: 1043 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Convert annotations to YOLOv8 format with proper scaling\n",
    "def convert_to_yolo_format(df, img_ids, output_dir_labels, original_dims_map, target_png_dims):\n",
    "    \"\"\"Convert bounding box annotations to YOLOv8 format with proper scaling\"\"\"\n",
    "    images_with_no_finding_label_file = 0\n",
    "    annotations_for_other_classes = 0\n",
    "    processed_class_counts = {i: 0 for i in range(15)} \n",
    "    \n",
    "    for img_id in tqdm(img_ids, desc=\"Converting annotations\"):\n",
    "        img_annotations_df = df[df['image_id'] == img_id]\n",
    "        \n",
    "        if len(img_annotations_df) == 0:\n",
    "            with open(os.path.join(output_dir_labels, f\"{img_id}.txt\"), 'w') as f:\n",
    "                pass \n",
    "            images_with_no_finding_label_file += 1\n",
    "            processed_class_counts[14] += 1 \n",
    "            continue\n",
    "            \n",
    "        orig_height, orig_width = original_dims_map.get(img_id, target_png_dims) \n",
    "        has_actual_ailment = any(img_annotations_df['class_id'] != 14)\n",
    "        \n",
    "        with open(os.path.join(output_dir_labels, f\"{img_id}.txt\"), 'w') as f:\n",
    "            if not has_actual_ailment:\n",
    "                images_with_no_finding_label_file += 1\n",
    "                processed_class_counts[14] += 1 \n",
    "                pass\n",
    "            else:\n",
    "                for _, row in img_annotations_df.iterrows():\n",
    "                    if row['class_id'] == 14:\n",
    "                        continue\n",
    "                    \n",
    "                    if pd.isna(row['x_min']) or pd.isna(row['y_min']) or pd.isna(row['x_max']) or pd.isna(row['y_max']) or pd.isna(row['class_id']):\n",
    "                        print(f\"Warning: NaN bbox/class_id found for image_id {img_id}, annotation skipped.\")\n",
    "                        continue\n",
    "\n",
    "                    actual_class_id = int(row['class_id'])\n",
    "                    annotations_for_other_classes += 1\n",
    "                    processed_class_counts[actual_class_id] += 1\n",
    "                    \n",
    "                    x_min, y_min, x_max, y_max = row['x_min'], row['y_min'], row['x_max'], row['y_max']\n",
    "                    scaled_bbox = scale_bbox([x_min, y_min, x_max, y_max], \n",
    "                                             (orig_height, orig_width), \n",
    "                                             target_png_dims)\n",
    "                    \n",
    "                    png_h, png_w = target_png_dims\n",
    "                    x_center_norm = ((scaled_bbox[0] + scaled_bbox[2]) / 2) / png_w\n",
    "                    y_center_norm = ((scaled_bbox[1] + scaled_bbox[3]) / 2) / png_h\n",
    "                    bbox_width_norm = (scaled_bbox[2] - scaled_bbox[0]) / png_w\n",
    "                    bbox_height_norm = (scaled_bbox[3] - scaled_bbox[1]) / png_h\n",
    "                    \n",
    "                    x_center_norm = max(0.0, min(x_center_norm, 1.0))\n",
    "                    y_center_norm = max(0.0, min(y_center_norm, 1.0))\n",
    "                    bbox_width_norm = max(0.001, min(bbox_width_norm, 1.0)) \n",
    "                    bbox_height_norm = max(0.001, min(bbox_height_norm, 1.0))\n",
    "                    \n",
    "                    f.write(f\"{actual_class_id} {x_center_norm} {y_center_norm} {bbox_width_norm} {bbox_height_norm}\\n\")\n",
    "    \n",
    "    print(f\"Created {images_with_no_finding_label_file} empty label files for 'No finding' images.\")\n",
    "    print(f\"Processed {annotations_for_other_classes} annotations for actual ailment classes (0-13).\")\n",
    "    print(\"Counts of each class_id written to label files (class 14 indicates empty files):\")\n",
    "    for class_id, count in processed_class_counts.items():\n",
    "        if count > 0 :\n",
    "            print(f\"Class {class_id}: {count} entries\")\n",
    "            \n",
    "    return images_with_no_finding_label_file, annotations_for_other_classes, processed_class_counts\n",
    "\n",
    "# Convert annotations for the initial train/val split\n",
    "print(\"\\nConverting initial training annotations to YOLOv8 format...\")\n",
    "_, _, train_label_class_counts_initial = convert_to_yolo_format(\n",
    "    train_df, \n",
    "    train_images, \n",
    "    f'{initial_dataset_dir_name}/labels/train',\n",
    "    original_dimensions,\n",
    "    (PNG_HEIGHT, PNG_WIDTH)\n",
    ")\n",
    "\n",
    "print(\"\\nConverting initial validation annotations to YOLOv8 format...\")\n",
    "_, _, val_label_class_counts_initial = convert_to_yolo_format(\n",
    "    train_df, \n",
    "    val_images, \n",
    "    f'{initial_dataset_dir_name}/labels/val',\n",
    "    original_dimensions,\n",
    "    (PNG_HEIGHT, PNG_WIDTH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6affc700-97f5-4a48-b7ba-889e88409d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying images for initial train/val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying initial training images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [09:02<00:00, 12.63it/s]\n",
      "Copying initial validation images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1716/1716 [04:08<00:00,  6.90it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCopying images for initial train/val split...\")\n",
    "image_source_dir = os.path.join(root_path, \"train\", \"train\") # Adjusted path\n",
    "\n",
    "for img_id in tqdm(train_images, desc=\"Copying initial training images\"):\n",
    "    src_path = os.path.join(image_source_dir, f\"{img_id}.png\")\n",
    "    dst_path = os.path.join(f'{initial_dataset_dir_name}/images/train', f\"{img_id}.png\")\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"Image not found: {src_path}\")\n",
    "\n",
    "for img_id in tqdm(val_images, desc=\"Copying initial validation images\"):\n",
    "    src_path = os.path.join(image_source_dir, f\"{img_id}.png\")\n",
    "    dst_path = os.path.join(f'{initial_dataset_dir_name}/images/val', f\"{img_id}.png\")\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"Image not found: {src_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3b4eb93-0e6f-4972-810d-c5aa60df2826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created dataset_initial.yaml for initial train/val split.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Create YOLOv8 dataset YAML file for initial train/val split (for tuning and gauge training)\n",
    "# This will be 'dataset_initial.yaml'\n",
    "data_yaml_initial_content = {\n",
    "    'path': os.path.abspath(initial_dataset_dir_name),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': 14, \n",
    "    'names': [\n",
    "        'Aortic enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly',\n",
    "        'Consolidation', 'ILD', 'Infiltration', 'Lung Opacity', 'Nodule/Mass',\n",
    "        'Other lesion', 'Pleural effusion', 'Pleural thickening', 'Pneumothorax',\n",
    "        'Pulmonary fibrosis'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save this as 'dataset_initial.yaml'\n",
    "path_to_initial_yaml = 'dataset_initial.yaml'\n",
    "with open(path_to_initial_yaml, 'w') as f:\n",
    "    yaml.dump(data_yaml_initial_content, f, sort_keys=False)\n",
    "print(f\"\\nCreated {path_to_initial_yaml} for initial train/val split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5271bb-e4a5-41a5-ad7f-dd0820830748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating class weights based on 24297 ailment annotations in initial training label files.\n",
      "\n",
      "Calculated initial class weights (normalized):\n",
      "Class Aortic enlargement (ID 0): 0.1581 (Count: 4342)\n",
      "Class Atelectasis (ID 1): 3.2679 (Count: 210)\n",
      "Class Calcification (ID 2): 1.0212 (Count: 672)\n",
      "Class Cardiomegaly (ID 3): 0.2106 (Count: 3258)\n",
      "Class Consolidation (ID 4): 1.6301 (Count: 421)\n",
      "Class ILD (ID 5): 0.9249 (Count: 742)\n",
      "Class Infiltration (ID 6): 0.7888 (Count: 870)\n",
      "Class Lung Opacity (ID 7): 0.3926 (Count: 1748)\n",
      "Class Nodule/Mass (ID 8): 0.3813 (Count: 1800)\n",
      "Class Other lesion (ID 9): 0.4382 (Count: 1566)\n",
      "Class Pleural effusion (ID 10): 0.3886 (Count: 1766)\n",
      "Class Pleural thickening (ID 11): 0.2007 (Count: 3420)\n",
      "Class Pneumothorax (ID 12): 3.9899 (Count: 172)\n",
      "Class Pulmonary fibrosis (ID 13): 0.2073 (Count: 3310)\n",
      "Initial class weights appended to dataset_initial.yaml\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Calculate class weights for balanced training (optional, for initial train set)\n",
    "# These weights could be used for the gauge training if desired, or let tuned HPs handle balancing.\n",
    "# We use train_label_class_counts_initial which reflects the initial training split.\n",
    "num_actual_classes_for_weights = data_yaml_initial_content['nc'] # Should be 14\n",
    "class_weights_initial = []\n",
    "\n",
    "total_ailment_annotations_in_initial_train = sum(\n",
    "    count for class_id, count in train_label_class_counts_initial.items() \n",
    "    if class_id < num_actual_classes_for_weights\n",
    ")\n",
    "\n",
    "print(f\"\\nCalculating class weights based on {total_ailment_annotations_in_initial_train} ailment annotations in initial training label files.\")\n",
    "\n",
    "if total_ailment_annotations_in_initial_train > 0:\n",
    "    for i in range(num_actual_classes_for_weights):  # Iterate 0-13\n",
    "        count = train_label_class_counts_initial.get(i, 0)\n",
    "        if count > 0:\n",
    "            weight = 1.0 / count \n",
    "        else:\n",
    "            weight = 1.0 \n",
    "        class_weights_initial.append(weight)\n",
    "\n",
    "    class_weights_initial = np.array(class_weights_initial)\n",
    "    class_weights_initial = (class_weights_initial / np.sum(class_weights_initial)) * num_actual_classes_for_weights\n",
    "    \n",
    "    print(\"\\nCalculated initial class weights (normalized):\")\n",
    "    for i, weight in enumerate(class_weights_initial):\n",
    "        print(f\"Class {data_yaml_initial_content['names'][i]} (ID {i}): {weight:.4f} (Count: {train_label_class_counts_initial.get(i, 0)})\")\n",
    "    \n",
    "    with open(path_to_initial_yaml, 'a') as f:\n",
    "        yaml.dump({\"weights\": class_weights_initial.tolist()}, f)\n",
    "        print(f\"Initial class weights appended to {path_to_initial_yaml}\")\n",
    "else:\n",
    "    print(\"No ailment annotations found in initial train set to calculate class weights. Using default weights (1.0 for all).\")\n",
    "    class_weights_initial = np.ones(num_actual_classes_for_weights).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9554eb3-6d36-4e7c-8a9b-b2228fc7d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created base hyperparameter file: hyp_medical_base.yaml\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Create a base hyperparameters file for medical imaging ('hyp_medical_base.yaml')\n",
    "# This can serve as a starting point or fallback if HP tuning fails or is skipped.\n",
    "hyp_medical_base = {\n",
    "    # Loss coefficients\n",
    "    \"box\": 7.5,\n",
    "    \"cls\": 0.5, \n",
    "    \"dfl\": 1.5,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    \"lr0\": 0.001,\n",
    "    \"lrf\": 0.01,\n",
    "    \"momentum\": 0.937,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"warmup_epochs\": 3.0,\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.1,\n",
    "    \n",
    "    # Augmentation settings\n",
    "    \"hsv_h\": 0.01,\n",
    "    \"hsv_s\": 0.2,\n",
    "    \"hsv_v\": 0.2,\n",
    "    \"degrees\": 5.0,\n",
    "    \"translate\": 0.1,\n",
    "    \"scale\": 0.3,\n",
    "    \"shear\": 0.0,\n",
    "    \"perspective\": 0.0,\n",
    "    \"flipud\": 0.0,\n",
    "    \"fliplr\": 0.5,\n",
    "    \"mosaic\": 0.3, # Will be adjusted by close_mosaic during training\n",
    "    \"mixup\": 0.0,\n",
    "    \"copy_paste\": 0.0,\n",
    "}\n",
    "\n",
    "path_to_base_hyp_yaml = 'hyp_medical_base.yaml'\n",
    "with open(path_to_base_hyp_yaml, 'w') as f:\n",
    "    yaml.dump(hyp_medical_base, f, sort_keys=False)\n",
    "print(f\"\\nCreated base hyperparameter file: {path_to_base_hyp_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8e1bde0-dd83-44e8-8090-c1c3c062ef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clearing GPU cache...\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nClearing GPU cache...\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2d6f239-eca4-46ea-b74a-eb923129c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle evaluation errors\n",
    "def safe_val(model_to_eval, data_path, split_name='val', imgsz_val=1024, batch_val=16, conf_val=0.25, iou_val=0.4, **kwargs):\n",
    "    \"\"\"Run validation with error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"Running validation: data={data_path}, split={split_name}, imgsz={imgsz_val}, batch={batch_val}, conf={conf_val}, iou={iou_val}\")\n",
    "        results = model_to_eval.val(\n",
    "            data=data_path, \n",
    "            split=split_name, \n",
    "            imgsz=imgsz_val, \n",
    "            batch=batch_val, \n",
    "            conf=conf_val, \n",
    "            iou=iou_val, \n",
    "            plots=True, # Enable plots for confusion matrix etc.\n",
    "            save_json=True, # Save JSON for pycocotools if needed\n",
    "            save_hybrid=True, # Save hybrid format (labels + predictions)\n",
    "            verbose=True, \n",
    "            **kwargs\n",
    "        )\n",
    "        return results\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError during validation: {e}. This can happen with inconsistent class indexing or metric calculation issues.\")\n",
    "        print(\"Attempting to proceed, but results might be incomplete.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during validation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "641fd282-8924-421d-a6e9-38822245c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.8M/38.8M [00:00<00:00, 64.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "Test = YOLO(\"yolo11m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f8d304c-98d8-4af2-9d06-a99d3e85fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: HYPERPARAMETER TUNING ---\n",
      "Starting hyperparameter tuning: 50 iterations, 15 epochs each.\n",
      "Using initial dataset for tuning: dataset_initial.yaml\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=chest_xray_runs/hp_tuning7'\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mðŸ’¡ Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/50 with hyperparameters: {'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "New https://pypi.org/project/ultralytics/8.3.142 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_initial.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=chest_xray_runs, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=chest_xray_runs/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=14\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1421818  ultralytics.nn.modules.head.Detect           [14, [256, 512, 512]]         \n",
      "YOLO11m summary: 231 layers, 20,063,802 parameters, 20,063,786 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1650.9Â±350.2 MB/s, size: 387.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset_initial_split/labels/train.cache... 6857 images, 4132 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1089.7Â±477.9 MB/s, size: 453.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset_initial_split/labels/val.cache... 1716 images, 1043 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1716/1716 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mchest_xray_runs/train2\u001b[0m\n",
      "Starting training for 15 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/15      7.91G      2.557      4.586      2.437        109        640:  19%|â–ˆâ–‰        | 81/429 [00:25<01:50,  3.14it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 1023, in <module>\n",
      "    entrypoint(debug=\"\")\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 981, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 793, in train\n",
      "    self.trainer.train()\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 212, in train\n",
      "    self._do_train(world_size)\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 386, in _do_train\n",
      "    loss, self.loss_items = self.model(batch)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 114, in forward\n",
      "    return self.loss(x, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 301, in loss\n",
      "    return self.criterion(preds, batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/utils/loss.py\", line 227, in __call__\n",
      "    imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize a YOLO model instance just for calling tune.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# The key is to pass 'model' and other critical args directly into tune().\u001b[39;00m\n\u001b[32m     17\u001b[39m tuner_instance = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo11m.pt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Create a base instance to call .tune()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m tune_results = \u001b[43mtuner_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#model='yolov11m.pt',             # *** 1. Explicitly specify the model to be tuned ***\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_to_initial_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Your dataset\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtuning_epochs_per_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# *** 2. This should now control trial epochs ***\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtuning_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchest_xray_runs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# For overall tuning project\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhp_tuning\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# For this specific tuning session\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# Crucial for tuner to evaluate HPs\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#cfg=path_to_base_hyp_yaml,    # *** 3. Temporarily REMOVE cfg to isolate issue ***\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m                                    \u001b[49m\u001b[38;5;66;43;03m# The tuner will evolve its own HPs.\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m                                    \u001b[49m\u001b[38;5;66;43;03m# You can merge results with your base HPs later if needed.\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Good to keep False for speed\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Good to keep False for speed & space\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=False,                # You might want verbose=True for debugging the first trial\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ... (rest of your code for finding path_to_best_hyperparameters_yaml) ...\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# (This part needs to correctly interpret tune_results based on your Ultralytics version)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tune_results, \u001b[33m'\u001b[39m\u001b[33msave_dir\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os.path.exists(os.path.join(tune_results.save_dir, \u001b[33m'\u001b[39m\u001b[33mbest_hyperparameters.yaml\u001b[39m\u001b[33m'\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:848\u001b[39m, in \u001b[36mModel.tune\u001b[39m\u001b[34m(self, use_ray, iterations, *args, **kwargs)\u001b[39m\n\u001b[32m    846\u001b[39m custom = {}  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[32m    847\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/site-packages/ultralytics/engine/tuner.py:196\u001b[39m, in \u001b[36mTuner.__call__\u001b[39m\u001b[34m(self, model, iterations, cleanup)\u001b[39m\n\u001b[32m    194\u001b[39m launch = [\u001b[38;5;28m__import__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msys\u001b[39m\u001b[33m\"\u001b[39m).executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33multralytics.cfg.__init__\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# workaround yolo not found\u001b[39;00m\n\u001b[32m    195\u001b[39m cmd = [*launch, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, *(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_args.items())]\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m return_code = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.returncode\n\u001b[32m    197\u001b[39m ckpt_file = weights_dir / (\u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (weights_dir / \u001b[33m\"\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlast.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m metrics = torch.load(ckpt_file)[\u001b[33m\"\u001b[39m\u001b[33mtrain_metrics\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mhedas/scratch_ssd/amunozbr/.micromamba/envs/amia-3.11/lib/python3.11/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = os.waitpid(\u001b[38;5;28mself\u001b[39m.pid, wait_flags)\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- STEP 1: HYPERPARAMETER TUNING ---\n",
    "print(\"\\n--- STEP 1: HYPERPARAMETER TUNING ---\")\n",
    "path_to_best_hyperparameters_yaml = path_to_base_hyp_yaml # Default fallback\n",
    "freeze_N_layers = 10\n",
    "\n",
    "try:\n",
    "    tuning_epochs_per_trial = 10  # Your desired epochs per trial\n",
    "    tuning_iterations = 15\n",
    "\n",
    "    print(f\"Starting hyperparameter tuning: {tuning_iterations} iterations, {tuning_epochs_per_trial} epochs each.\")\n",
    "    print(f\"Using initial dataset for tuning: {path_to_initial_yaml}\")\n",
    "    \n",
    "    tuner_instance = YOLO(\"yolo11m.pt\") # Create a base instance to call .tune()\n",
    "\n",
    "    tune_results = tuner_instance.tune(\n",
    "        data=path_to_initial_yaml,      \n",
    "        epochs=tuning_epochs_per_trial, \n",
    "        iterations=tuning_iterations,\n",
    "        optimizer='AdamW',\n",
    "        project='chest_xray_runs',      \n",
    "        name='hp_tuning',               \n",
    "        exist_ok=True,\n",
    "        val=True,                       \n",
    "        plots=False,                    \n",
    "        save=False,                     \n",
    "        device=device,\n",
    "        # verbose=False,                \n",
    "        freeze=freeze_N_layers\n",
    "    )\n",
    "    \n",
    "    # ... (rest of your code for finding path_to_best_hyperparameters_yaml) ...\n",
    "    # (This part needs to correctly interpret tune_results based on your Ultralytics version)\n",
    "    if hasattr(tune_results, 'save_dir') and os.path.exists(os.path.join(tune_results.save_dir, 'best_hyperparameters.yaml')):\n",
    "        path_to_best_hyperparameters_yaml = os.path.join(tune_results.save_dir, 'best_hyperparameters.yaml')\n",
    "    elif hasattr(tune_results, 'best_hyp_yaml') and os.path.exists(tune_results.best_hyp_yaml): # another possible attribute\n",
    "         path_to_best_hyperparameters_yaml = tune_results.best_hyp_yaml\n",
    "    else:\n",
    "        presumed_path = os.path.join('chest_xray_runs', 'hp_tuning', 'best_hyperparameters.yaml')\n",
    "        if os.path.exists(presumed_path):\n",
    "            path_to_best_hyperparameters_yaml = presumed_path\n",
    "        else:\n",
    "            print(f\"Warning: 'best_hyperparameters.yaml' not found automatically.\")\n",
    "            path_to_best_hyperparameters_yaml = path_to_base_hyp_yaml\n",
    "\n",
    "\n",
    "    print(f\"Tuning completed. Using hyperparameters from: {path_to_best_hyperparameters_yaml}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during hyperparameter tuning: {e}\")\n",
    "    print(f\"Falling back to using base HPs: {path_to_base_hyp_yaml}\")\n",
    "    path_to_best_hyperparameters_yaml = path_to_base_hyp_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232e466-1038-4e22-8e7a-15fcbbae1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: \"GAUGE\" TRAINING & THRESHOLD FINDING (using tuned HPs on initial train/val split) ---\n",
    "print(\"\\n--- STEP 2: GAUGE TRAINING & THRESHOLD FINDING ---\")\n",
    "\n",
    "# Define parameters for the gauge training run\n",
    "gauge_training_epochs = 150 # Set a high number of epochs; early stopping will determine the actual count\n",
    "gauge_patience = 20       # Patience for early stopping\n",
    "\n",
    "model_gauge = YOLO('yolov8m.pt') # Start with a fresh pre-trained model\n",
    "print(f\"Starting gauge training with HPs from '{path_to_best_hyperparameters_yaml}' for up to {gauge_training_epochs} epochs.\")\n",
    "print(f\"Using dataset for gauge training: {path_to_initial_yaml}\")\n",
    "\n",
    "results_gauge = model_gauge.train(\n",
    "    data=path_to_initial_yaml,    # Uses 'dataset_initial.yaml' (initial train/val split)\n",
    "    epochs=gauge_training_epochs,\n",
    "    patience=gauge_patience,\n",
    "    batch=8,                      # Adjust batch size based on your VRAM\n",
    "    imgsz=1024,\n",
    "    device=device,                # Use the auto-detected device\n",
    "    val=True,                     # Crucial for early stopping & getting validation metrics\n",
    "    amp=True,                     # Automatic Mixed Precision training\n",
    "    cfg=path_to_best_hyperparameters_yaml, # Use HPs from tuning (or base HPs if tuning failed)\n",
    "    optimizer='AdamW',            # This might be overridden by 'cfg' if specified there\n",
    "    project='chest_xray_runs',    # Main project for all runs\n",
    "    name='gauge_train_run',       # Specific name for this gauge training run\n",
    "    exist_ok=True,                # Overwrite if previous run with same name exists\n",
    "    cos_lr=True,                  # Use cosine learning rate scheduler (often in tuned HPs)\n",
    "    close_mosaic=10,              # Disable mosaic in last N epochs (often in tuned HPs)\n",
    "    freeze=freeze_N_layers,       # Apply freezing strategy\n",
    "    multi_scale=True,             # Enable multi-scale training if part of your strategy\n",
    "    verbose=True,\n",
    "    # cls_pw=class_weights_initial.tolist() # Optionally pass initial class weights if not handled by HPs/autobalance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f031d-915a-420e-9956-8b10a014697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of epochs for the final run based on gauge training\n",
    "epochs_for_final_run = results_gauge.epoch + 1 # Default to total epochs completed if no early stopping or best_epoch not found\n",
    "if hasattr(results_gauge, 'best_epoch') and results_gauge.best_epoch is not None and results_gauge.best_epoch > 0 :\n",
    "    # best_epoch is 0-indexed epoch number of the best model found during validation\n",
    "    epochs_for_final_run = results_gauge.best_epoch + 1\n",
    "    print(f\"Gauge training stopped early or best epoch identified. Optimal epoch for best.pt: {results_gauge.best_epoch}.\")\n",
    "else:\n",
    "    print(f\"Gauge training completed {results_gauge.epoch + 1} epochs.\")\n",
    "print(f\"Derived optimal number of epochs for final training run: {epochs_for_final_run}\")\n",
    "\n",
    "# Path to the best model weights from the gauge training\n",
    "path_to_gauge_model_weights = os.path.join('chest_xray_runs', 'gauge_train_run', 'weights', 'best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7e20d-7d55-4a07-8ac2-3c1ad294e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find optimal confidence threshold using the gauge model ---\n",
    "optimal_threshold_glob = 0.25 # Default value\n",
    "threshold_df_gauge = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(path_to_gauge_model_weights):\n",
    "    print(f\"\\nLoading gauge model from {path_to_gauge_model_weights} for threshold finding...\")\n",
    "    model_for_thresholding = YOLO(path_to_gauge_model_weights)\n",
    "    \n",
    "    print(\"\\n--- FINDING OPTIMAL CONFIDENCE THRESHOLD (using Gauge Model on initial val split) ---\")\n",
    "    # Using IoU @ 0.4 as per competition guidelines for Pascal VOC mAP\n",
    "    conf_thresholds_search_gauge = np.linspace(0.01, 0.50, 20).tolist() # Search range for confidence\n",
    "    threshold_results_list_gauge = []\n",
    "\n",
    "    for conf_val_search in tqdm(conf_thresholds_search_gauge, desc=\"Evaluating confidence thresholds (Gauge Model)\"):\n",
    "        results_conf_search_gauge = safe_val(\n",
    "            model_for_thresholding,\n",
    "            data_path=path_to_initial_yaml, # Validate on the initial validation set\n",
    "            split_name='val',\n",
    "            imgsz_val=1024,\n",
    "            batch_val=max(4, 8 // 2), # Adjust batch size for validation if needed\n",
    "            device=device,\n",
    "            conf_val=conf_val_search,\n",
    "            iou_val=0.4,   # PASCAL VOC metric often uses IoU > 0.4 for mAP@0.4 (equivalent to map50 if iou_thres for map50 is 0.4)\n",
    "                               # Note: Ultralytics map50 is typically IoU@0.5. For IoU@0.4, direct metrics might be needed or check val_results.\n",
    "                               # We will use map50 (mAP@0.5) and F1 for optimization here as it's standard output.\n",
    "                               # If competition uses IoU@0.4, ensure val uses correct IoU for reported mAP (e.g. val_results.box.map(iou_thres=0.4))\n",
    "        )\n",
    "        \n",
    "        if results_conf_search_gauge and hasattr(results_conf_search_gauge, 'box'):\n",
    "            # results_conf_search_gauge.box.map50 is mAP@0.5\n",
    "            # results_conf_search_gauge.box.map is mAP@0.5:0.95\n",
    "            mean_precision_gauge = results_conf_search_gauge.box.mp\n",
    "            mean_recall_gauge = results_conf_search_gauge.box.mr\n",
    "            f1_val_gauge = 0\n",
    "            if mean_precision_gauge > 0 and mean_recall_gauge > 0:\n",
    "                 f1_val_gauge = 2 * (mean_precision_gauge * mean_recall_gauge) / (mean_precision_gauge + mean_recall_gauge)\n",
    "            elif hasattr(results_conf_search_gauge.box, 'f1') and len(results_conf_search_gauge.box.f1) > 0: # Fallback\n",
    "                 f1_val_gauge = np.mean(results_conf_search_gauge.box.f1)\n",
    "\n",
    "            threshold_results_list_gauge.append({\n",
    "                'Conf': conf_val_search,\n",
    "                'mAP50': results_conf_search_gauge.box.map50, # mAP at IoU=0.5\n",
    "                'mAP50_95': results_conf_search_gauge.box.map, \n",
    "                'MeanPrecision': mean_precision_gauge,\n",
    "                'MeanRecall': mean_recall_gauge,\n",
    "                'MeanF1': f1_val_gauge\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Error or no results evaluating gauge model with confidence threshold {conf_val_search:.3f}\")\n",
    "\n",
    "    if threshold_results_list_gauge:\n",
    "        threshold_df_gauge = pd.DataFrame(threshold_results_list_gauge)\n",
    "        print(\"\\nGauge Model Confidence Threshold Analysis (Metrics @ IoU for mAP50 i.e. 0.5):\")\n",
    "        print(threshold_df_gauge.to_string(index=False))\n",
    "\n",
    "        threshold_df_gauge.to_csv('confidence_threshold_analysis_gauge_model.csv', index=False)\n",
    "        print(\"Gauge model confidence threshold analysis saved to confidence_threshold_analysis_gauge_model.csv\")\n",
    "\n",
    "        # Optimize for MeanF1, fallback to mAP50 if F1 is zero everywhere\n",
    "        metric_to_optimize_for_threshold_gauge = 'MeanF1' \n",
    "        if not (threshold_df_gauge[metric_to_optimize_for_threshold_gauge] > 0).any():\n",
    "            metric_to_optimize_for_threshold_gauge = 'mAP50' # mAP@0.5\n",
    "\n",
    "        if metric_to_optimize_for_threshold_gauge in threshold_df_gauge.columns and \\\n",
    "           (threshold_df_gauge[metric_to_optimize_for_threshold_gauge] > 0).any() :\n",
    "            optimal_idx_gauge = threshold_df_gauge[metric_to_optimize_for_threshold_gauge].idxmax()\n",
    "            optimal_threshold_glob = threshold_df_gauge.loc[optimal_idx_gauge, 'Conf']\n",
    "            optimal_metric_val_gauge = threshold_df_gauge.loc[optimal_idx_gauge, metric_to_optimize_for_threshold_gauge]\n",
    "            print(f\"\\nOptimal confidence threshold from Gauge Model (based on {metric_to_optimize_for_threshold_gauge}): {optimal_threshold_glob:.4f} (Value: {optimal_metric_val_gauge:.4f})\")\n",
    "        else:\n",
    "            print(f\"Could not determine optimal threshold for gauge model based on {metric_to_optimize_for_threshold_gauge}, using default: {optimal_threshold_glob}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(threshold_df_gauge['Conf'], threshold_df_gauge['MeanPrecision'], marker='o', linestyle='-', label='Mean Precision')\n",
    "        plt.plot(threshold_df_gauge['Conf'], threshold_df_gauge['MeanRecall'], marker='s', linestyle='-', label='Mean Recall')\n",
    "        plt.plot(threshold_df_gauge['Conf'], threshold_df_gauge['MeanF1'], marker='^', linestyle='-', label='Mean F1-Score')\n",
    "        plt.plot(threshold_df_gauge['Conf'], threshold_df_gauge['mAP50'], marker='x', linestyle='-', label='mAP@0.5')\n",
    "        plt.axvline(x=optimal_threshold_glob, color='grey', linestyle='--', label=f'Optimal Conf ({optimal_threshold_glob:.3f})')\n",
    "        plt.xlabel('Confidence Threshold', fontsize=12)\n",
    "        plt.ylabel('Metric Value', fontsize=12)\n",
    "        plt.title('Gauge Model: Metrics vs. Confidence Threshold', fontsize=14)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle=':', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confidence_threshold_metrics_gauge_model.png', dpi=150)\n",
    "        print(\"Gauge model confidence threshold analysis plot saved to confidence_threshold_metrics_gauge_model.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Gauge model confidence threshold evaluation failed to produce results. Using default threshold.\")\n",
    "else:\n",
    "    print(f\"Gauge model weights not found at {path_to_gauge_model_weights}. Skipping optimal threshold search, using default: {optimal_threshold_glob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf860f58-e91f-47ad-82b6-96343a416763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARE COMBINED DATASET (TRAIN+VAL) FOR FINAL TRAINING ---\n",
    "print(\"\\n--- PREPARING COMBINED DATASET (TRAIN+VAL) FOR FINAL TRAINING ---\")\n",
    "\n",
    "# train_images and val_images are from In[9]\n",
    "all_image_ids_final_combined_data = train_images + val_images\n",
    "print(f\"Total images for final combined training: {len(all_image_ids_final_combined_data)}\")\n",
    "\n",
    "final_combined_dataset_root_name = 'yolov8_dataset_final_combined' # Directory name for the combined dataset\n",
    "\n",
    "# Create directories for the combined dataset\n",
    "os.makedirs(f'{final_combined_dataset_root_name}/images/train_all', exist_ok=True)\n",
    "os.makedirs(f'{final_combined_dataset_root_name}/labels/train_all', exist_ok=True)\n",
    "\n",
    "print(\"\\nConverting combined (train+val) annotations to YOLOv8 format for final training...\")\n",
    "# Re-use the convert_to_yolo_format function from In[10]\n",
    "# train_df is the full training dataframe from In[4]\n",
    "# original_dimensions is from In[5]\n",
    "# PNG_HEIGHT, PNG_WIDTH are from In[6]\n",
    "_, _, final_combined_label_class_counts = convert_to_yolo_format(\n",
    "    train_df, \n",
    "    all_image_ids_final_combined_data,\n",
    "    f'{final_combined_dataset_root_name}/labels/train_all', # Output to the new combined labels directory\n",
    "    original_dimensions,\n",
    "    (PNG_HEIGHT, PNG_WIDTH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac644c81-ccfe-469a-92af-a6bd83439596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCopying all images (initial train + initial val) for final training...\")\n",
    "for img_id in tqdm(all_image_ids_final_combined_data, desc=\"Copying combined images for final training\"):\n",
    "    src_path = os.path.join(image_source_dir, f\"{img_id}.png\")\n",
    "    dst_path = os.path.join(f'{final_combined_dataset_root_name}/images/train_all', f\"{img_id}.png\")\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"Image not found during combined dataset copy: {src_path}\")\n",
    "\n",
    "# Create the YAML file for the final combined dataset\n",
    "path_to_final_combined_yaml = 'dataset_final_combined.yaml'\n",
    "data_yaml_final_combined_content = {\n",
    "    'path': os.path.abspath(final_combined_dataset_root_name), # Absolute path to the combined dataset root\n",
    "    'train': 'images/train_all',  # Path to training images (all of them)\n",
    "    'val': 'images/train_all',    # For final training, val can point to train or be an empty dir if val=False in train()\n",
    "    'nc': data_yaml_initial_content['nc'],\n",
    "    'names': data_yaml_initial_content['names']\n",
    "}\n",
    "with open(path_to_final_combined_yaml, 'w') as f:\n",
    "    yaml.dump(data_yaml_final_combined_content, f, sort_keys=False)\n",
    "print(f\"Created '{path_to_final_combined_yaml}' for the final combined training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a605619-9cc1-41b8-8b11-c01153ea7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: FINAL TRAINING ON ALL DATA (TRAIN+VAL) ---\n",
    "print(\"\\n--- STEP 3: FINAL TRAINING ON ALL COMBINED (TRAIN+VAL) DATA ---\")\n",
    "\n",
    "model_final_submission = YOLO('yolov8m.pt')\n",
    "print(f\"Starting final training on all combined data with HPs from '{path_to_best_hyperparameters_yaml}'.\")\n",
    "print(f\"Training for {epochs_for_final_run} epochs.\")\n",
    "print(f\"Using combined dataset: {path_to_final_combined_yaml}\")\n",
    "\n",
    "results_final_submission = model_final_submission.train(\n",
    "    data=path_to_final_combined_yaml,    # YAML file for the combined (train+val) dataset\n",
    "    epochs=epochs_for_final_run,         # Number of epochs determined from gauge training\n",
    "    patience=0,                          # No early stopping needed here, train for the full determined duration\n",
    "    batch=8,                             # Consistent batch size (adjust if needed)\n",
    "    imgsz=1024,\n",
    "    device=device,                       # Use the auto-detected device\n",
    "    val=False,                           # Set to False as we are training on all data and not validating against a separate set\n",
    "                                         # If val=True, and YAML points val to train_all, metrics will be on training data.\n",
    "    amp=True,                            # Automatic Mixed Precision training\n",
    "    cfg=path_to_best_hyperparameters_yaml, # Use HPs from tuning (or base HPs)\n",
    "    optimizer='AdamW',                   # This might be overridden by 'cfg'\n",
    "    project='chest_xray_runs',           # Main project for all runs\n",
    "    name='final_submission_model_run',   # Specific name for this final submission model training run\n",
    "    exist_ok=True,                       # Overwrite if previous run with same name exists\n",
    "    cos_lr=True,                         # Use cosine learning rate scheduler (often in tuned HPs)\n",
    "    close_mosaic=10 if epochs_for_final_run > 10 else 0, # Disable mosaic in last N epochs (consistent with HPs)\n",
    "    freeze=freeze_N_layers,              # Apply consistent freezing strategy\n",
    "    multi_scale=True,                    # Enable multi-scale training (consistent with strategy)\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Path to the weights of the final model trained on all data\n",
    "# If val=False during training, 'last.pt' is usually the one to use.\n",
    "# If val=True (and val pointed to train data), 'best.pt' might also be considered, but 'last.pt' represents training for full epochs.\n",
    "path_to_submission_model_weights = os.path.join('chest_xray_runs', 'final_submission_model_run', 'weights', 'last.pt')\n",
    "print(f\"Final model for submission (trained on all data) should be at: {path_to_submission_model_weights}\")\n",
    "\n",
    "# Verify if the path exists, if not, maybe 'best.pt' was saved due to some internal logic even with val=False\n",
    "if not os.path.exists(path_to_submission_model_weights):\n",
    "    alt_path = os.path.join('chest_xray_runs', 'final_submission_model_run', 'weights', 'best.pt')\n",
    "    if os.path.exists(alt_path):\n",
    "        path_to_submission_model_weights = alt_path\n",
    "        print(f\"'last.pt' not found, using 'best.pt' instead from final run: {path_to_submission_model_weights}\")\n",
    "    else:\n",
    "        print(f\"CRITICAL WARNING: Final model weights ('last.pt' or 'best.pt') not found at expected location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38153d99-7c76-4e75-b77f-cdd6c0513242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale bounding boxes back to original DICOM dimensions\n",
    "def scale_bbox_back(bbox_xyxy, png_dims_tuple, original_dims_tuple):\n",
    "    \"\"\"Scale bounding box coordinates from PNG dimensions back to original dimensions\"\"\"\n",
    "    png_height, png_width = png_dims_tuple\n",
    "    orig_height, orig_width = original_dims_tuple\n",
    "    \n",
    "    if png_width == 0 or png_height == 0: # Avoid division by zero if PNG dims are invalid\n",
    "        print(f\"Warning: Invalid PNG dimensions ({png_height}x{png_width}) for scaling back. Returning unscaled box.\")\n",
    "        return [int(round(c)) for c in bbox_xyxy]\n",
    "\n",
    "    # Scale factors\n",
    "    width_scale_factor = orig_width / png_width\n",
    "    height_scale_factor = orig_height / png_height\n",
    "    \n",
    "    # Scale coordinates back to original dimensions\n",
    "    x_min_orig = int(round(bbox_xyxy[0] * width_scale_factor))\n",
    "    y_min_orig = int(round(bbox_xyxy[1] * height_scale_factor))\n",
    "    x_max_orig = int(round(bbox_xyxy[2] * width_scale_factor))\n",
    "    y_max_orig = int(round(bbox_xyxy[3] * height_scale_factor))\n",
    "    \n",
    "    # Ensure x_max > x_min and y_max > y_min after scaling and rounding\n",
    "    # Also ensure coordinates are within original image bounds\n",
    "    x_min_orig = max(0, min(x_min_orig, orig_width -1))\n",
    "    y_min_orig = max(0, min(y_min_orig, orig_height -1))\n",
    "    x_max_orig = max(0, min(x_max_orig, orig_width -1))\n",
    "    y_max_orig = max(0, min(y_max_orig, orig_height -1))\n",
    "\n",
    "    if x_max_orig <= x_min_orig: x_max_orig = x_min_orig + 1\n",
    "    if y_max_orig <= y_min_orig: y_max_orig = y_min_orig + 1\n",
    "        \n",
    "    # Final check to ensure max is not beyond image boundary if min was already at boundary-1\n",
    "    x_max_orig = min(x_max_orig, orig_width)\n",
    "    y_max_orig = min(y_max_orig, orig_height)\n",
    "\n",
    "    return [x_min_orig, y_min_orig, x_max_orig, y_max_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653f36e-1782-4dbb-8d15-6f5cfe02f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 4: TEST SET INFERENCE AND SUBMISSION FILE GENERATION ---\n",
    "print(\"\\n--- TEST SET INFERENCE ---\")\n",
    "\n",
    "submission_predictions_list = [] \n",
    "test_images_processed_count = 0\n",
    "test_no_detections_count = 0\n",
    "submission_file_path = 'submission.csv' \n",
    "\n",
    "# path_to_submission_model_weights is from In[20] (final model trained on all data)\n",
    "# optimal_threshold_glob is from In[18] (determined using gauge model)\n",
    "# original_dimensions is from In[5] (loaded from img_size.csv)\n",
    "# data_yaml_initial_content contains class names (from In[12])\n",
    "# PNG_HEIGHT, PNG_WIDTH are from In[6]\n",
    "\n",
    "model_for_submission = None\n",
    "if os.path.exists(path_to_submission_model_weights):\n",
    "    print(f\"Loading final submission model from: {path_to_submission_model_weights}\")\n",
    "    model_for_submission = YOLO(path_to_submission_model_weights)\n",
    "else:\n",
    "    print(f\"ERROR: Final submission model weights not found at {path_to_submission_model_weights}. Cannot perform inference.\")\n",
    "\n",
    "if model_for_submission is not None:\n",
    "    print(f\"Starting inference on test set using optimal confidence: {optimal_threshold_glob:.4f}\")\n",
    "    \n",
    "    # Path to test images and sample submission file\n",
    "    # root_path is from In[3]\n",
    "    test_data_path = os.path.join(root_path, \"test\", \"test\") # Ensure this is the correct path to test PNGs\n",
    "    sample_submission_df = pd.read_csv(os.path.join(root_path, \"sample_submission.csv\"))\n",
    "    \n",
    "    print(f\"Found {len(sample_submission_df)} images in sample_submission.csv for test inference.\")\n",
    "\n",
    "    for img_id_test in tqdm(sample_submission_df['image_id'], desc=\"Running inference on test set\"):\n",
    "        current_img_path = os.path.join(test_data_path, f\"{img_id_test}.png\")\n",
    "        \n",
    "        if os.path.exists(current_img_path):\n",
    "            test_images_processed_count += 1\n",
    "            \n",
    "            # Get original dimensions for this test image\n",
    "            orig_dims_test = original_dimensions.get(img_id_test)\n",
    "            if orig_dims_test is None:\n",
    "                print(f\"Warning: Original dimensions not found for test image {img_id_test}. Using PNG dimensions ({PNG_HEIGHT}x{PNG_WIDTH}) as fallback for scaling back.\")\n",
    "                orig_dims_test = (PNG_HEIGHT, PNG_WIDTH) # Fallback, though this should ideally not happen\n",
    "            \n",
    "            # Run inference\n",
    "            # augment=True enables Test-Time Augmentation (TTA) which can improve results\n",
    "            # iou for NMS, can be tuned. 0.45 is a reasonable default.\n",
    "            inference_results = model_for_submission(\n",
    "                current_img_path, \n",
    "                conf=optimal_threshold_glob, \n",
    "                iou=0.45,  # NMS IoU threshold\n",
    "                imgsz=1024, \n",
    "                augment=True, # Enable TTA\n",
    "                verbose=False # Reduce log spam during loop\n",
    "            )\n",
    "            \n",
    "            # Results are usually a list, even for a single image\n",
    "            results_for_image = inference_results[0] if isinstance(inference_results, list) else inference_results\n",
    "\n",
    "            pred_boxes_xyxy_png = results_for_image.boxes.xyxy.cpu().numpy()  # Coords on 1024x1024 scale\n",
    "            pred_scores = results_for_image.boxes.conf.cpu().numpy()\n",
    "            pred_classes_indices = results_for_image.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            if len(pred_boxes_xyxy_png) == 0:\n",
    "                test_no_detections_count += 1\n",
    "                # Format for \"No finding\": class_id 14, score 1.0, and dummy box (0 0 1 1)\n",
    "                submission_predictions_list.append(f\"{img_id_test},14 1.0 0 0 1 1\")\n",
    "            else:\n",
    "                img_prediction_strings_parts = []\n",
    "                for box_coords_png, score_val, class_idx in zip(pred_boxes_xyxy_png, pred_scores, pred_classes_indices):\n",
    "                    # Ensure predicted class index is valid\n",
    "                    if class_idx >= data_yaml_initial_content['nc']: \n",
    "                        print(f\"Warning: Predicted class index {class_idx} is out of bounds for image {img_id_test}. Skipping this box.\")\n",
    "                        continue\n",
    "\n",
    "                    # Scale box back to original DICOM dimensions\n",
    "                    scaled_box_orig_dims = scale_bbox_back(box_coords_png, (PNG_HEIGHT, PNG_WIDTH), orig_dims_test)\n",
    "                    \n",
    "                    # Format: \"class_id score x_min y_min x_max y_max\"\n",
    "                    img_prediction_strings_parts.append(\n",
    "                        f\"{class_idx} {score_val:.6f} {scaled_box_orig_dims[0]} {scaled_box_orig_dims[1]} {scaled_box_orig_dims[2]} {scaled_box_orig_dims[3]}\"\n",
    "                    )\n",
    "                \n",
    "                if img_prediction_strings_parts: # If there were any valid predictions for this image\n",
    "                    submission_predictions_list.append(f\"{img_id_test},{' '.join(img_prediction_strings_parts)}\")\n",
    "                else: # If all predictions were filtered out (e.g., due to invalid class index)\n",
    "                    test_no_detections_count += 1\n",
    "                    submission_predictions_list.append(f\"{img_id_test},14 1.0 0 0 1 1\")\n",
    "\n",
    "        else: # If test image file does not exist\n",
    "            print(f\"Test image not found: {current_img_path}. Adding as 'No finding' to submission.\")\n",
    "            # Per competition rules, if an image is missing, it might need a specific format.\n",
    "            submission_predictions_list.append(f\"{img_id_test},14 1.0 0 0 1 1\") # Default for missing images\n",
    "\n",
    "    print(f\"\\nProcessed {test_images_processed_count} test images.\")\n",
    "    print(f\"Number of images with no detections (classified as 'No finding'): {test_no_detections_count}\")\n",
    "    if test_images_processed_count > 0:\n",
    "        print(f\"Percentage of 'No finding' classifications in processed images: {(test_no_detections_count/test_images_processed_count)*100:.2f}%\")\n",
    "\n",
    "    # Create the submission CSV file\n",
    "    with open(submission_file_path, 'w') as f:\n",
    "        f.write(\"image_id,PredictionString\\n\") # Header\n",
    "        for pred_line in submission_predictions_list:\n",
    "            f.write(f\"{pred_line}\\n\")\n",
    "\n",
    "    print(f\"\\nSubmission file created at: {submission_file_path}\")\n",
    "else:\n",
    "    print(\"\\nSkipping Test Set Inference: Final submission model not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe7415-ede5-4773-8412-bdf484d356be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZE SOME TEST PREDICTIONS (Optional) ---\n",
    "print(\"\\n--- CREATING TEST PREDICTION VISUALIZATIONS (OPTIONAL) ---\")\n",
    "\n",
    "# model_for_submission is from In[22]\n",
    "# optimal_threshold_glob from In[18]\n",
    "# data_yaml_initial_content for class names from In[12]\n",
    "# test_data_path for loading images from In[22]\n",
    "\n",
    "if model_for_submission is not None and test_images_processed_count > 0:\n",
    "    try:\n",
    "        num_images_to_visualize = min(5, test_images_processed_count) # Visualize up to 5 images\n",
    "        \n",
    "        # Ensure we sample from image IDs that were actually processed and exist in sample_submission_df\n",
    "        processed_img_ids_for_viz = [\n",
    "            entry.split(',')[0] for entry in submission_predictions_list \n",
    "            if entry.split(',')[0] in sample_submission_df['image_id'].values\n",
    "        ]\n",
    "        if not processed_img_ids_for_viz : # Fallback if list is empty for some reason\n",
    "             processed_img_ids_for_viz = sample_submission_df['image_id'].sample(n=num_images_to_visualize, replace=False).tolist() \\\n",
    "                                         if len(sample_submission_df) >= num_images_to_visualize else sample_submission_df['image_id'].tolist()\n",
    "\n",
    "\n",
    "        sample_ids_for_viz = np.random.choice(\n",
    "            processed_img_ids_for_viz, \n",
    "            size=min(num_images_to_visualize, len(processed_img_ids_for_viz)), # Ensure sample size isn't too large\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Generating visualizations for {len(sample_ids_for_viz)} random test images...\")\n",
    "        \n",
    "        for img_id_viz in sample_ids_for_viz:\n",
    "            img_path_viz = os.path.join(test_data_path, f\"{img_id_viz}.png\")\n",
    "            if not os.path.exists(img_path_viz):\n",
    "                print(f\"Skipping visualization for missing image: {img_path_viz}\")\n",
    "                continue\n",
    "            \n",
    "            # Load image using PIL\n",
    "            pil_img = Image.open(img_path_viz).convert(\"RGB\") # Ensure 3 channels for matplotlib if it's grayscale\n",
    "            \n",
    "            # Run inference again for this image (TTA can be off for single viz clarity)\n",
    "            viz_results_list = model_for_submission(\n",
    "                img_path_viz, \n",
    "                conf=optimal_threshold_glob, \n",
    "                iou=0.45, \n",
    "                imgsz=1024, \n",
    "                augment=False # TTA off for single visualization clarity\n",
    "            )\n",
    "            viz_results_img = viz_results_list[0] if isinstance(viz_results_list, list) else viz_results_list\n",
    "\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            plt.imshow(np.array(pil_img)) # Display using matplotlib\n",
    "            plt.title(f\"Test Image: {img_id_viz} (Conf: {optimal_threshold_glob:.3f})\", fontsize=14)\n",
    "            ax = plt.gca() # Get current axes\n",
    "\n",
    "            viz_boxes_png = viz_results_img.boxes.xyxy.cpu().numpy() # BBoxes on 1024x1024 scale\n",
    "            viz_scores = viz_results_img.boxes.conf.cpu().numpy()\n",
    "            viz_classes_indices = viz_results_img.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            if len(viz_boxes_png) == 0:\n",
    "                ax.text(pil_img.width * 0.05, pil_img.height * 0.05, \n",
    "                        \"Prediction: No finding\", fontsize=14, color='red', \n",
    "                        bbox=dict(facecolor='white', alpha=0.7, edgecolor='red'))\n",
    "            else:\n",
    "                for box_png, score_viz, cls_idx_viz in zip(viz_boxes_png, viz_scores, viz_classes_indices):\n",
    "                    x_min_png, y_min_png, x_max_png, y_max_png = box_png\n",
    "                    \n",
    "                    class_name_viz = data_yaml_initial_content['names'][cls_idx_viz] \\\n",
    "                                     if cls_idx_viz < len(data_yaml_initial_content['names']) \\\n",
    "                                     else f\"Unknown Class {cls_idx_viz}\"\n",
    "                    \n",
    "                    # Create a rectangle patch (coordinates are for the displayed image, which is 1024x1024)\n",
    "                    rect = plt.Rectangle((x_min_png, y_min_png), x_max_png - x_min_png, y_max_png - y_min_png,\n",
    "                                         fill=False, edgecolor=plt.cm.get_cmap('tab20', data_yaml_initial_content['nc'])(cls_idx_viz % 20), \n",
    "                                         linewidth=2.5)\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Add label text\n",
    "                    label_text = f\"{class_name_viz}: {score_viz:.2f}\"\n",
    "                    ax.text(x_min_png, y_min_png - 10, label_text, \n",
    "                            color='white', fontsize=10, \n",
    "                            bbox=dict(facecolor=plt.cm.get_cmap('tab20', data_yaml_initial_content['nc'])(cls_idx_viz % 20), \n",
    "                                      alpha=0.8, edgecolor='none', pad=1))\n",
    "            \n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            viz_save_path = f\"test_prediction_visualization_{img_id_viz}.png\"\n",
    "            plt.savefig(viz_save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"Saved visualization: {viz_save_path}\")\n",
    "            plt.close() # Close plot to free memory\n",
    "            \n",
    "    except Exception as e_viz:\n",
    "        print(f\"Could not generate test visualizations: {e_viz}\")\n",
    "else:\n",
    "    print(\"\\nSkipping Test Visualizations: Final submission model not loaded or no test images processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffc56a-05d3-4caf-90ac-2c04ae30b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[24]:\n",
    "# --- PER-CLASS METRICS AND CONFUSION MATRIX (from Gauge Model evaluation) ---\n",
    "# This refers to the validation performed on the Gauge Model (Step 2), as it used a proper validation set.\n",
    "# The 'val_results' needed here would be from the `safe_val` call during threshold finding for the gauge model,\n",
    "# or from the `results_gauge.val()` if you ran it explicitly after gauge training.\n",
    "# For simplicity, let's assume we want to re-run validation on the Gauge Model with the optimal threshold to get these plots.\n",
    "\n",
    "print(\"\\n--- ANALYSIS OF GAUGE MODEL ON INITIAL VALIDATION SET ---\")\n",
    "# path_to_gauge_model_weights from In[18]\n",
    "# optimal_threshold_glob from In[18]\n",
    "# path_to_initial_yaml from In[12] ('dataset_initial.yaml')\n",
    "\n",
    "if os.path.exists(path_to_gauge_model_weights):\n",
    "    print(f\"Loading gauge model from {path_to_gauge_model_weights} for final analysis on validation set.\")\n",
    "    gauge_model_for_analysis = YOLO(path_to_gauge_model_weights)\n",
    "    \n",
    "    print(f\"Running final validation on gauge model using Conf: {optimal_threshold_glob:.4f} and IoU: 0.4\")\n",
    "    # Note: plots=True in safe_val will generate confusion_matrix.png, P_curve.png, R_curve.png, PR_curve.png etc.\n",
    "    # in the run directory (e.g., runs/detect/valX/)\n",
    "    gauge_val_results_final_analysis = safe_val(\n",
    "        gauge_model_for_analysis,\n",
    "        data_path=path_to_initial_yaml,\n",
    "        split_name='val',\n",
    "        imgsz_val=1024,\n",
    "        batch_val=max(4, 8 // 2),\n",
    "        conf_val=optimal_threshold_glob, # Use the determined optimal threshold\n",
    "        iou_val=0.4, # Competition relevant IoU for mAP\n",
    "        project='chest_xray_runs',\n",
    "        name='gauge_model_final_val_analysis' # Specific directory for these validation results\n",
    "    )\n",
    "\n",
    "    if gauge_val_results_final_analysis and hasattr(gauge_val_results_final_analysis, 'box'):\n",
    "        print(\"\\nGauge Model Final Validation Metrics (IoU @ 0.4):\")\n",
    "        # map50 here is actually mAP@0.4 because iou_val=0.4 was passed to safe_val,\n",
    "        # and safe_val passes iou_val to model.val(iou=iou_val) which then influences how map50 is calculated if iou is primary.\n",
    "        # However, ultralytics .val() uses a range for .map and typically .map50 is specifically IoU=0.5.\n",
    "        # For specific IoU mAP: results.box.maps[iou_threshold_index] where iou_threshold_index corresponds to 0.4 in the iou_vector.\n",
    "        # For simplicity, we'll report map50 (often IoU@0.5) and the mean P, R.\n",
    "        print(f\"mAP@0.5 (map50): {gauge_val_results_final_analysis.box.map50:.4f}\")\n",
    "        print(f\"mAP@0.5:0.95 (map): {gauge_val_results_final_analysis.box.map:.4f}\")\n",
    "        mean_p_gauge_final = gauge_val_results_final_analysis.box.mp\n",
    "        mean_r_gauge_final = gauge_val_results_final_analysis.box.mr\n",
    "        print(f\"Mean Precision: {mean_p_gauge_final:.4f}\")\n",
    "        print(f\"Mean Recall: {mean_r_gauge_final:.4f}\")\n",
    "        if mean_p_gauge_final > 0 and mean_r_gauge_final > 0:\n",
    "            mean_f1_gauge_final = 2 * (mean_p_gauge_final * mean_r_gauge_final) / (mean_p_gauge_final + mean_r_gauge_final)\n",
    "            print(f\"Mean F1-Score (calculated): {mean_f1_gauge_final:.4f}\")\n",
    "        \n",
    "        if hasattr(gauge_val_results_final_analysis, 'save_dir'):\n",
    "            print(f\"Validation plots (like confusion matrix) for Gauge Model saved in: {gauge_val_results_final_analysis.save_dir}\")\n",
    "            # Example: cm_path = os.path.join(gauge_val_results_final_analysis.save_dir, 'confusion_matrix.png')\n",
    "        \n",
    "        # Per-class metrics\n",
    "        class_names_viz = data_yaml_initial_content['names']\n",
    "        metrics_data_gauge = []\n",
    "        print(\"\\nGauge Model Per-Class Metrics (from its validation run, typically IoU@0.5 for AP50):\")\n",
    "        for i in range(len(class_names_viz)):\n",
    "            try:\n",
    "                # class_result(i) returns (p[i], r[i], ap50[i], ap[i])\n",
    "                p_cls, r_cls, ap50_cls, _ = gauge_val_results_final_analysis.box.class_result(i)\n",
    "                metrics_data_gauge.append({\n",
    "                    'Class ID': i, 'Class Name': class_names_viz[i],\n",
    "                    'AP@0.5': float(ap50_cls), # AP50 is mAP for the class at IoU 0.5\n",
    "                    'Precision': float(p_cls), 'Recall': float(r_cls)\n",
    "                })\n",
    "            except IndexError: # Should not happen if nc is correct\n",
    "                 metrics_data_gauge.append({'Class ID': i, 'Class Name': class_names_viz[i], 'AP@0.5': 0.0, 'Precision': 0.0, 'Recall': 0.0})\n",
    "\n",
    "        metrics_df_gauge = pd.DataFrame(metrics_data_gauge)\n",
    "        print(metrics_df_gauge.to_string(index=False))\n",
    "        metrics_df_gauge.to_csv('per_class_metrics_gauge_model.csv', index=False)\n",
    "        print(\"Per-class metrics for gauge model saved to per_class_metrics_gauge_model.csv\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not retrieve final validation metrics for the Gauge Model.\")\n",
    "else:\n",
    "    print(\"Gauge model weights not found. Skipping final analysis of gauge model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04be53-682c-4255-bd2c-75bfd6f15e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- FULL PIPELINE EXECUTION COMPLETE ---\")\n",
    "if os.path.exists(path_to_submission_model_weights):\n",
    "    print(f\"Final model for submission is located at: {path_to_submission_model_weights}\")\n",
    "    print(f\"Optimal confidence threshold used for test inference: {optimal_threshold_glob:.4f}\")\n",
    "    print(f\"Submission file should be at: {submission_file_path}\")\n",
    "else:\n",
    "    print(\"Pipeline completed, but the final submission model might not have been generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 FINAL",
   "language": "python",
   "name": "amia-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
