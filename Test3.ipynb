{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b402cd-9756-4cd6-b584-9293ab73ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics version: 8.3.130\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd866a67-e17c-4134-b5ca-107409a835ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: GPU-65d5f9c5-4882-dda2-e605-d1f9d08ac724\n",
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "GPU 0: NVIDIA RTX 4000 Ada Generation\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"Not set\"))\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Check GPU access\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322861c8-0520-49a2-816e-2cec75043f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/data/mhedas/common/challenge_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628745a4-1862-43b0-af36-8693b0f32cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45925 annotations\n",
      "Number of unique images: 8573\n",
      "\n",
      "Class distribution:\n",
      "Class 0: 5481 annotations\n",
      "Class 1: 255 annotations\n",
      "Class 2: 851 annotations\n",
      "Class 3: 4046 annotations\n",
      "Class 4: 519 annotations\n",
      "Class 5: 904 annotations\n",
      "Class 6: 1097 annotations\n",
      "Class 7: 2188 annotations\n",
      "Class 8: 2324 annotations\n",
      "Class 9: 1945 annotations\n",
      "Class 10: 2190 annotations\n",
      "Class 11: 4308 annotations\n",
      "Class 12: 195 annotations\n",
      "Class 13: 4097 annotations\n",
      "Class 14: 15525 annotations\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load and preprocess the data\n",
    "train_df = pd.read_csv(os.path.join(root_path, \"train.csv\"))\n",
    "img_dims_df = pd.read_csv(os.path.join(root_path, \"img_size.csv\"))\n",
    "train_df = train_df.merge(img_dims_df, on='image_id', how='left')\n",
    "\n",
    "print(f\"Loaded {len(train_df)} annotations\")\n",
    "print(f\"Number of unique images: {train_df['image_id'].nunique()}\")\n",
    "\n",
    "# Print class distribution\n",
    "class_counts = train_df['class_id'].value_counts().sort_index()\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ee91b1-88ef-4763-a26c-25dbd02078a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dimensions for 15000 images\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a mapping of original image dimensions from img_size.csv\n",
    "def load_image_dimensions(csv_path):\n",
    "    \"\"\"Load image dimensions from CSV file\"\"\"\n",
    "    img_dimensions = {}\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Store dimensions as (height, width)\n",
    "        img_dimensions[row['image_id']] = (int(row['dim0']), int(row['dim1']))\n",
    "    \n",
    "    return img_dimensions\n",
    "\n",
    "original_dimensions = load_image_dimensions(os.path.join(root_path, \"img_size.csv\"))\n",
    "print(f\"Loaded dimensions for {len(original_dimensions)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cb96b7-bd58-4877-9c2e-9df56e0dbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PNG dimensions - all PNGs are 1024x1024\n",
    "PNG_HEIGHT, PNG_WIDTH = 1024, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d56c0b0-d944-4044-a5ce-87ba8d87d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Function to scale bounding box coordinates\n",
    "def scale_bbox(bbox, original_dims, new_dims):\n",
    "    \"\"\"\n",
    "    Scale bounding box coordinates from original dimensions to new dimensions\n",
    "    \n",
    "    Args:\n",
    "        bbox (list/array): Bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "        original_dims (tuple): Original image dimensions (height, width)\n",
    "        new_dims (tuple): New image dimensions (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        list: Scaled bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = original_dims\n",
    "    new_height, new_width = new_dims\n",
    "    \n",
    "    # Scale factors\n",
    "    width_scale = new_width / orig_width\n",
    "    height_scale = new_height / orig_height\n",
    "    \n",
    "    # Scale coordinates\n",
    "    x_min = bbox[0] * width_scale\n",
    "    y_min = bbox[1] * height_scale\n",
    "    x_max = bbox[2] * width_scale\n",
    "    y_max = bbox[3] * height_scale\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d42560f-e805-4166-bc0a-dcd697dd20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create directories for YOLOv8 dataset format\n",
    "os.makedirs('yolov8_dataset/images/train', exist_ok=True)\n",
    "os.makedirs('yolov8_dataset/images/val', exist_ok=True)\n",
    "os.makedirs('yolov8_dataset/labels/train', exist_ok=True)\n",
    "os.makedirs('yolov8_dataset/labels/val', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100f0d0e-6724-4e14-84d1-ed4626c90505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing stratified split to maintain class distribution...\n",
      "Class 0: 2365 unique images\n",
      "Class 1: 167 unique images\n",
      "Class 2: 385 unique images\n",
      "Class 3: 1746 unique images\n",
      "Class 4: 325 unique images\n",
      "Class 5: 341 unique images\n",
      "Class 6: 532 unique images\n",
      "Class 7: 1132 unique images\n",
      "Class 8: 705 unique images\n",
      "Class 9: 965 unique images\n",
      "Class 10: 909 unique images\n",
      "Class 11: 1689 unique images\n",
      "Class 12: 79 unique images\n",
      "Class 13: 1388 unique images\n",
      "Class 14: 5175 unique images\n",
      "Training images: 6857\n",
      "Validation images: 1716\n",
      "\n",
      "Training set class distribution:\n",
      "Class 0: 4361 annotations (11.8%)\n",
      "Class 1: 215 annotations (0.6%)\n",
      "Class 2: 655 annotations (1.8%)\n",
      "Class 3: 3251 annotations (8.8%)\n",
      "Class 4: 417 annotations (1.1%)\n",
      "Class 5: 743 annotations (2.0%)\n",
      "Class 6: 884 annotations (2.4%)\n",
      "Class 7: 1772 annotations (4.8%)\n",
      "Class 8: 1954 annotations (5.3%)\n",
      "Class 9: 1518 annotations (4.1%)\n",
      "Class 10: 1764 annotations (4.8%)\n",
      "Class 11: 3460 annotations (9.4%)\n",
      "Class 12: 155 annotations (0.4%)\n",
      "Class 13: 3295 annotations (8.9%)\n",
      "Class 14: 12411 annotations (33.7%)\n",
      "\n",
      "Validation set class distribution:\n",
      "Class 0: 1120 annotations (12.3%)\n",
      "Class 1: 40 annotations (0.4%)\n",
      "Class 2: 196 annotations (2.2%)\n",
      "Class 3: 795 annotations (8.8%)\n",
      "Class 4: 102 annotations (1.1%)\n",
      "Class 5: 161 annotations (1.8%)\n",
      "Class 6: 213 annotations (2.3%)\n",
      "Class 7: 416 annotations (4.6%)\n",
      "Class 8: 370 annotations (4.1%)\n",
      "Class 9: 427 annotations (4.7%)\n",
      "Class 10: 426 annotations (4.7%)\n",
      "Class 11: 848 annotations (9.3%)\n",
      "Class 12: 40 annotations (0.4%)\n",
      "Class 13: 802 annotations (8.8%)\n",
      "Class 14: 3114 annotations (34.3%)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Split data into training and validation sets with stratification\n",
    "print(\"\\nPerforming stratified split to maintain class distribution...\")\n",
    "\n",
    "# Get unique image IDs for each class\n",
    "class_image_ids = {}\n",
    "for class_id in range(15):  # 0-14 classes\n",
    "    class_image_ids[class_id] = set(train_df[train_df['class_id'] == class_id]['image_id'].unique())\n",
    "\n",
    "# Count how many images have each class\n",
    "for class_id, img_ids in class_image_ids.items():\n",
    "    print(f\"Class {class_id}: {len(img_ids)} unique images\")\n",
    "\n",
    "# Create a stratified split based on the presence of rarer classes\n",
    "all_image_ids = train_df['image_id'].unique()\n",
    "rare_classes = [0, 2, 5, 6, 12, 13]  # Classes with fewer instances based on your counts\n",
    "images_with_rare_classes = set()\n",
    "for class_id in rare_classes:\n",
    "    images_with_rare_classes.update(class_image_ids[class_id])\n",
    "\n",
    "# Split both rare and common images with the same ratio\n",
    "rare_images = list(images_with_rare_classes)\n",
    "common_images = list(set(all_image_ids) - images_with_rare_classes)\n",
    "\n",
    "# Split rare and common images separately with the same ratio\n",
    "rare_train, rare_val = train_test_split(rare_images, test_size=0.2, random_state=42)\n",
    "common_train, common_val = train_test_split(common_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the splits\n",
    "train_images = rare_train + common_train\n",
    "val_images = rare_val + common_val\n",
    "\n",
    "print(f\"Training images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(val_images)}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "train_df_subset = train_df[train_df['image_id'].isin(train_images)]\n",
    "val_df_subset = train_df[train_df['image_id'].isin(val_images)]\n",
    "\n",
    "train_class_counts = train_df_subset['class_id'].value_counts().sort_index()\n",
    "val_class_counts = val_df_subset['class_id'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "for class_id, count in train_class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} annotations ({count/sum(train_class_counts)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "for class_id, count in val_class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} annotations ({count/sum(val_class_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722617bb-5709-46ae-b23e-216a18f252a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting annotations to YOLOv8 format with coordinate scaling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [01:59<00:00, 57.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4137 'No finding' images and 24444 annotations for other classes\n",
      "Class 0: 4361 annotations\n",
      "Class 1: 215 annotations\n",
      "Class 2: 655 annotations\n",
      "Class 3: 3251 annotations\n",
      "Class 4: 417 annotations\n",
      "Class 5: 743 annotations\n",
      "Class 6: 884 annotations\n",
      "Class 7: 1772 annotations\n",
      "Class 8: 1954 annotations\n",
      "Class 9: 1518 annotations\n",
      "Class 10: 1764 annotations\n",
      "Class 11: 3460 annotations\n",
      "Class 12: 155 annotations\n",
      "Class 13: 3295 annotations\n",
      "Class 14: 4137 annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1716/1716 [00:29<00:00, 57.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1038 'No finding' images and 5956 annotations for other classes\n",
      "Class 0: 1120 annotations\n",
      "Class 1: 40 annotations\n",
      "Class 2: 196 annotations\n",
      "Class 3: 795 annotations\n",
      "Class 4: 102 annotations\n",
      "Class 5: 161 annotations\n",
      "Class 6: 213 annotations\n",
      "Class 7: 416 annotations\n",
      "Class 8: 370 annotations\n",
      "Class 9: 427 annotations\n",
      "Class 10: 426 annotations\n",
      "Class 11: 848 annotations\n",
      "Class 12: 40 annotations\n",
      "Class 13: 802 annotations\n",
      "Class 14: 1038 annotations\n",
      "Training set: 4137 'No finding' images, 24444 annotations for other classes\n",
      "Validation set: 1038 'No finding' images, 5956 annotations for other classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Convert annotations to YOLOv8 format with proper scaling\n",
    "def convert_to_yolo_format(df, img_ids, output_dir, original_dims, png_dims):\n",
    "    \"\"\"Convert bounding box annotations to YOLOv8 format with proper scaling\"\"\"\n",
    "    no_finding_count = 0\n",
    "    other_class_count = 0\n",
    "    classes_count = {i: 0 for i in range(15)}  # Count for each class\n",
    "    \n",
    "    for img_id in tqdm(img_ids, desc=\"Converting annotations\"):\n",
    "        img_annotations = df[df['image_id'] == img_id]\n",
    "        \n",
    "        # Skip if no annotations found\n",
    "        if len(img_annotations) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get original dimensions for this image\n",
    "        orig_height, orig_width = original_dims.get(img_id, (3000, 3000))  # Default if not found\n",
    "        \n",
    "        # Check if this is a \"No finding\" image (all annotations are class 14)\n",
    "        is_no_finding = all(img_annotations['class_id'] == 14)\n",
    "        \n",
    "        # Create label file\n",
    "        with open(os.path.join(output_dir, f\"{img_id}.txt\"), 'w') as f:\n",
    "            # For \"No finding\" images, we'll create an empty label file\n",
    "            # This is the proper way to handle background/negative samples in YOLO\n",
    "            if is_no_finding:\n",
    "                no_finding_count += 1\n",
    "                classes_count[14] += 1\n",
    "                # Empty file indicates no objects (YOLO understands this)\n",
    "                pass\n",
    "            else:\n",
    "                for _, row in img_annotations.iterrows():\n",
    "                    # Skip the \"No finding\" class entries in mixed images\n",
    "                    if row['class_id'] == 14:\n",
    "                        continue\n",
    "                        \n",
    "                    other_class_count += 1\n",
    "                    classes_count[row['class_id']] += 1\n",
    "                    \n",
    "                    # Get bounding box coordinates from the dataframe\n",
    "                    x_min, y_min, x_max, y_max = row['x_min'], row['y_min'], row['x_max'], row['y_max']\n",
    "                    \n",
    "                    # Skip rows with NaN values\n",
    "                    if pd.isna(x_min) or pd.isna(y_min) or pd.isna(x_max) or pd.isna(y_max):\n",
    "                        continue\n",
    "                    \n",
    "                    # Scale bbox coordinates from original dimensions to PNG dimensions\n",
    "                    scaled_bbox = scale_bbox([x_min, y_min, x_max, y_max], \n",
    "                                            (orig_height, orig_width), \n",
    "                                            png_dims)\n",
    "                    \n",
    "                    # Convert to YOLO format (normalized coordinates)\n",
    "                    x_center = ((scaled_bbox[0] + scaled_bbox[2]) / 2) / png_dims[1]\n",
    "                    y_center = ((scaled_bbox[1] + scaled_bbox[3]) / 2) / png_dims[0]\n",
    "                    bbox_width = (scaled_bbox[2] - scaled_bbox[0]) / png_dims[1]\n",
    "                    bbox_height = (scaled_bbox[3] - scaled_bbox[1]) / png_dims[0]\n",
    "                    \n",
    "                    # Make sure values are within [0,1] range\n",
    "                    x_center = max(0, min(x_center, 1))\n",
    "                    y_center = max(0, min(y_center, 1))\n",
    "                    bbox_width = max(0, min(bbox_width, 1))\n",
    "                    bbox_height = max(0, min(bbox_height, 1))\n",
    "                    \n",
    "                    # Write to file (class_id, x_center, y_center, width, height)\n",
    "                    f.write(f\"{row['class_id']} {x_center} {y_center} {bbox_width} {bbox_height}\\n\")\n",
    "    \n",
    "    print(f\"Processed {no_finding_count} 'No finding' images and {other_class_count} annotations for other classes\")\n",
    "    for class_id, count in classes_count.items():\n",
    "        if count > 0:\n",
    "            print(f\"Class {class_id}: {count} annotations\")\n",
    "    \n",
    "    return no_finding_count, other_class_count, classes_count\n",
    "\n",
    "# Convert annotations with proper scaling\n",
    "print(\"Converting annotations to YOLOv8 format with coordinate scaling...\")\n",
    "train_no_finding, train_other, train_class_counts = convert_to_yolo_format(\n",
    "    train_df, \n",
    "    train_images, \n",
    "    'yolov8_dataset/labels/train',\n",
    "    original_dimensions,\n",
    "    (PNG_HEIGHT, PNG_WIDTH)\n",
    ")\n",
    "\n",
    "val_no_finding, val_other, val_class_counts = convert_to_yolo_format(\n",
    "    train_df, \n",
    "    val_images, \n",
    "    'yolov8_dataset/labels/val',\n",
    "    original_dimensions,\n",
    "    (PNG_HEIGHT, PNG_WIDTH)\n",
    ")\n",
    "\n",
    "print(f\"Training set: {train_no_finding} 'No finding' images, {train_other} annotations for other classes\")\n",
    "print(f\"Validation set: {val_no_finding} 'No finding' images, {val_other} annotations for other classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33321279-151e-416c-83d4-1afdc36bb43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1716"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074c3193-9945-47f5-885b-62e1ed72ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images to YOLOv8 directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying training images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6857/6857 [08:23<00:00, 13.61it/s]\n",
      "Copying validation images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1716/1716 [02:12<00:00, 12.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Copy images to YOLOv8 directories\n",
    "print(\"Copying images to YOLOv8 directories...\")\n",
    "for img_id in tqdm(train_images, desc=\"Copying training images\"):\n",
    "    src_path = os.path.join(root_path, \"train\", \"train\", f\"{img_id}.png\")\n",
    "    dst_path = os.path.join('yolov8_dataset/images/train', f\"{img_id}.png\")\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"Image not found: {src_path}\")\n",
    "\n",
    "for img_id in tqdm(val_images, desc=\"Copying validation images\"):\n",
    "    src_path = os.path.join(root_path, \"train\", \"train\", f\"{img_id}.png\")\n",
    "    dst_path = os.path.join('yolov8_dataset/images/val', f\"{img_id}.png\")\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"Image not found: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2de3caa0-d833-451d-83ed-932701b7c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset.yaml configuration file\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Create YOLOv8 dataset YAML file\n",
    "data_yaml = {\n",
    "    'path': os.path.abspath('yolov8_dataset'),  # dataset root dir\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': 14,  # number of classes (we only detect 14 classes, \"No finding\" is handled differently)\n",
    "    'names': [\n",
    "        'Aortic enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly',\n",
    "        'Consolidation', 'ILD', 'Infiltration', 'Lung Opacity', 'Nodule/Mass',\n",
    "        'Other lesion', 'Pleural effusion', 'Pleural thickening', 'Pneumothorax',\n",
    "        'Pulmonary fibrosis'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('dataset.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "print(\"Created dataset.yaml configuration file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10fbe64a-3e24-4ee6-9295-68b1df15aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights for balanced training:\n",
      "Class 0: 0.1533\n",
      "Class 1: 3.1103\n",
      "Class 2: 1.0209\n",
      "Class 3: 0.2057\n",
      "Class 4: 1.6036\n",
      "Class 5: 0.9000\n",
      "Class 6: 0.7565\n",
      "Class 7: 0.3774\n",
      "Class 8: 0.3422\n",
      "Class 9: 0.4405\n",
      "Class 10: 0.3791\n",
      "Class 11: 0.1933\n",
      "Class 12: 4.3143\n",
      "Class 13: 0.2029\n",
      "Class weights saved to dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for balanced training\n",
    "class_weights = []\n",
    "total_annotations = sum(train_class_counts.values())\n",
    "for class_id in range(14):  # Only include 0-13 classes (exclude \"No finding\" class)\n",
    "    count = train_class_counts.get(class_id, 0)\n",
    "    if count > 0:\n",
    "        # Inverse frequency weighting\n",
    "        weight = total_annotations / (14 * count)\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    class_weights.append(weight)\n",
    "\n",
    "# Normalize weights to have a mean of 1\n",
    "class_weights = np.array(class_weights)\n",
    "class_weights = class_weights / np.mean(class_weights)\n",
    "\n",
    "print(\"\\nClass weights for balanced training:\")\n",
    "for class_id, weight in enumerate(class_weights):\n",
    "    print(f\"Class {class_id}: {weight:.4f}\")\n",
    "\n",
    "# Save class weights to file\n",
    "with open('dataset.yaml', 'a') as f:\n",
    "    yaml.dump({\"weights\": class_weights.tolist()}, f)\n",
    "print(\"Class weights saved to dataset.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3359d583-9f98-41cd-8738-e0468668ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Create hyperparameters file for medical imaging\n",
    "hyp = {\n",
    "    # Loss coefficients\n",
    "    \"box\": 7.5,  # Box loss weight\n",
    "    \"cls\": 0.5,  # Classification loss weight\n",
    "    \"dfl\": 1.5,  # Distribution focal loss\n",
    "    \n",
    "    # Optimizer settings\n",
    "    \"lr0\": 0.001,  # Initial learning rate\n",
    "    \"lrf\": 0.01,   # Final learning rate factor\n",
    "    \"momentum\": 0.937,  # SGD momentum/Adam beta1\n",
    "    \"weight_decay\": 0.0005,  # Optimizer weight decay\n",
    "    \"warmup_epochs\": 3.0,  # Warmup epochs\n",
    "    \"warmup_momentum\": 0.8,  # Warmup momentum\n",
    "    \"warmup_bias_lr\": 0.1,  # Warmup bias learning rate\n",
    "    \n",
    "    # Augmentation settings (optimized for medical imaging)\n",
    "    \"hsv_h\": 0.01,  # Hue augmentation\n",
    "    \"hsv_s\": 0.1,   # Saturation augmentation\n",
    "    \"hsv_v\": 0.1,   # Value augmentation\n",
    "    \"degrees\": 0.0,  # Rotation (disabled for medical orientation)\n",
    "    \"translate\": 0.1,  # Translation\n",
    "    \"scale\": 0.1,   # Scale\n",
    "    \"shear\": 0.0,   # Shear\n",
    "    \"perspective\": 0.0,  # Perspective\n",
    "    \"flipud\": 0.0,  # Vertical flip\n",
    "    \"fliplr\": 0.5,  # Horizontal flip\n",
    "    \"mosaic\": 0.3,  # Mosaic (reduced)\n",
    "    \"mixup\": 0.0,   # Mixup (disabled)\n",
    "    \"copy_paste\": 0.0,  # Copy-paste\n",
    "}\n",
    "\n",
    "# Save hyperparameters to a file\n",
    "with open('hyp_medical.yaml', 'w') as f:\n",
    "    yaml.dump(hyp, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981fcec3-3095-46ad-afcc-61692c08b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache...\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before training\n",
    "print(\"Clearing GPU cache...\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f07272e9-5d46-4eb4-a593-d01445dd44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle evaluation errors\n",
    "def safe_val(model, data, split='val', **kwargs):\n",
    "    \"\"\"Run validation with error handling for missing keys\"\"\"\n",
    "    try:\n",
    "        results = model.val(data=data, split=split, **kwargs)\n",
    "        return results\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError during validation: {e}\")\n",
    "        # If error occurs, try with default parameters\n",
    "        print(\"Retrying with default parameters...\")\n",
    "        results = model.val(data=data, split=split)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during validation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b97c731-ebe6-41d6-bf8b-5c901afe880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 1: TRAINING WITH FROZEN BACKBONE ---\n",
      "New https://pypi.org/project/ultralytics/8.3.131 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=hyp_medical.yaml, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], half=False, hsv_h=0.01, hsv_s=0.1, hsv_v=0.1, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=0.3, multi_scale=False, name=yolov8m_stage1, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=chest_xray, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=chest_xray/yolov8m_stage1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=14\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3783802  ultralytics.nn.modules.head.Detect           [14, [192, 384, 576]]         \n",
      "Model summary: 169 layers, 25,864,426 parameters, 25,864,410 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.2.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.2.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.3.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.3.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 416.8Â±63.8 MB/s, size: 443.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/train... 8573 images, 5175 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8573/8573 [00:03<00:00, 2255.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/train.cache\n",
      "WARNING âš ï¸ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.2Â±0.1 ms, read: 340.2Â±49.2 MB/s, size: 436.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:02<00:00, 2235.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache\n",
      "Plotting labels to chest_xray/yolov8m_stage1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mchest_xray/yolov8m_stage1\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/25      1.88G      1.798      3.974       1.96          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [03:03<00:00, 11.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.314      0.069     0.0762     0.0361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/25      2.44G      1.712      3.123       1.91          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:57<00:00, 12.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.387      0.101      0.112     0.0526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/25      2.45G      1.654      2.862      1.847          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.519      0.117      0.125       0.06\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/25      2.59G      1.607      2.621      1.794          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.294      0.173      0.137     0.0658\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/25      2.59G      1.567      2.451      1.751          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.284      0.178       0.15      0.075\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/25      2.59G      1.544      2.281       1.72          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.331      0.178      0.157     0.0783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/25      2.59G      1.506      2.238      1.683          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.397      0.197       0.18     0.0866\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/25       2.6G      1.486      2.109      1.659          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.424      0.186      0.205      0.103\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/25      2.64G      1.463       1.97      1.634          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.336      0.226      0.204      0.102\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/25      2.64G      1.438      1.905      1.605          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.328      0.243      0.212      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/25      2.64G      1.416      1.843      1.582          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.335      0.274      0.236      0.116\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/25      2.64G      1.397      1.807      1.562          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.337      0.302       0.25      0.123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/25      2.64G      1.371      1.726      1.541          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.338       0.31      0.266      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/25      2.64G      1.354      1.643      1.524          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.371       0.33      0.289      0.146\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/25      2.64G      1.329      1.573      1.501          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.373      0.341      0.297      0.149\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/25      2.64G      1.308      1.525       1.48          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.388      0.347      0.304      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/25      2.64G      1.286      1.487      1.462          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.416      0.347      0.324      0.169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/25      2.64G      1.264      1.442      1.445          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.417      0.357       0.34      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/25      2.64G      1.243      1.398      1.424          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.436      0.362      0.352      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/25      2.64G      1.224      1.351      1.412          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.431      0.376      0.359      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/25      2.64G      1.204      1.317      1.394          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.455      0.373      0.368      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/25      2.64G      1.192      1.294      1.385          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:57<00:00, 12.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.45      0.375      0.371      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/25      2.64G      1.181      1.267      1.376          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.446      0.385      0.374      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/25      2.64G      1.172      1.257      1.371          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:56<00:00, 12.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.46      0.388      0.379      0.209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/25      2.64G      1.165      1.243      1.364          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [02:57<00:00, 12.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.46      0.389      0.379       0.21\n",
      "\n",
      "25 epochs completed in 1.816 hours.\n",
      "Optimizer stripped from chest_xray/yolov8m_stage1/weights/last.pt, 52.1MB\n",
      "Optimizer stripped from chest_xray/yolov8m_stage1/weights/best.pt, 52.1MB\n",
      "\n",
      "Validating chest_xray/yolov8m_stage1/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:13<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.46      0.388      0.379       0.21\n",
      "    Aortic enlargement       1749       4053       0.68      0.585      0.649      0.437\n",
      "           Atelectasis        128        198      0.433      0.157      0.211      0.108\n",
      "         Calcification        281        625      0.364      0.312      0.286      0.144\n",
      "          Cardiomegaly       1287       2970       0.72      0.531      0.632      0.453\n",
      "         Consolidation        229        380      0.412      0.358      0.342      0.184\n",
      "                   ILD        265        711      0.589      0.254      0.363      0.202\n",
      "          Infiltration        395        815      0.414      0.323       0.32      0.159\n",
      "          Lung Opacity        836       1606      0.403      0.428      0.344      0.168\n",
      "           Nodule/Mass        534       1740      0.525      0.345      0.371      0.205\n",
      "          Other lesion        728       1510      0.338      0.299      0.264      0.127\n",
      "      Pleural effusion        671       1585      0.372      0.528      0.432      0.203\n",
      "    Pleural thickening       1261       3187      0.289      0.464      0.318      0.138\n",
      "          Pneumothorax         57        143      0.549      0.417      0.405      0.231\n",
      "    Pulmonary fibrosis       1021       2995       0.35      0.436      0.363      0.174\n",
      "Speed: 0.2ms preprocess, 10.0ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mchest_xray/yolov8m_stage1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 11: First stage of training - frozen backbone\n",
    "print(\"\\n--- STAGE 1: TRAINING WITH FROZEN BACKBONE ---\")\n",
    "model_stage1 = YOLO('yolov8m.pt')  # Start with pretrained model\n",
    "\n",
    "# Get number of layers in model\n",
    "num_layers = len(model_stage1.model.model)\n",
    "backbone_layers = list(range(10))  # Freeze first 10 layers (backbone)\n",
    "\n",
    "# First stage training with frozen backbone\n",
    "results_stage1 = model_stage1.train(\n",
    "    data='dataset.yaml',\n",
    "    epochs=25,              # Shorter first stage\n",
    "    patience=10,            # Early stopping\n",
    "    batch=4,                # Batch size\n",
    "    imgsz=1024,             # Image size\n",
    "    device=0,               # GPU device\n",
    "    val=True,               # Validate during training\n",
    "    amp=True,               # Mixed precision training\n",
    "    pretrained=True,        # Use pretrained weights\n",
    "    cfg='hyp_medical.yaml', # Custom hyperparameters\n",
    "    optimizer='AdamW',      # Optimizer\n",
    "    project='chest_xray',   # Project name\n",
    "    name='yolov8m_stage1',  # Run name\n",
    "    exist_ok=True,          # Overwrite existing run\n",
    "    cos_lr=True,            # Cosine LR scheduler\n",
    "    close_mosaic=10,        # Disable mosaic in last 10 epochs\n",
    "    freeze=backbone_layers, # Freeze backbone layers\n",
    "    rect=True,              # Rectangular training\n",
    "    verbose=True,           # Verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15322361-88e8-4283-be88-37790223aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating first stage model...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1660.9Â±353.4 MB/s, size: 435.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.74      0.194       0.47      0.302\n",
      "    Aortic enlargement       1749       4053      0.806      0.389      0.616      0.457\n",
      "           Atelectasis        128        198        0.5     0.0455       0.27      0.172\n",
      "         Calcification        281        625      0.735      0.115       0.43      0.272\n",
      "          Cardiomegaly       1287       2970      0.832      0.408      0.642      0.507\n",
      "         Consolidation        229        380      0.775     0.0816      0.427      0.284\n",
      "                   ILD        265        711      0.852      0.105      0.478      0.333\n",
      "          Infiltration        395        815      0.701      0.144      0.413       0.25\n",
      "          Lung Opacity        836       1606      0.646      0.189      0.405       0.23\n",
      "           Nodule/Mass        534       1740      0.755      0.209      0.486      0.331\n",
      "          Other lesion        728       1510      0.727      0.141      0.437      0.254\n",
      "      Pleural effusion        671       1585      0.799      0.216      0.506      0.279\n",
      "    Pleural thickening       1261       3187      0.691      0.176      0.425      0.212\n",
      "          Pneumothorax         57        143       0.84      0.294      0.582      0.385\n",
      "    Pulmonary fibrosis       1021       2995      0.706       0.21      0.463      0.259\n",
      "Speed: 0.3ms preprocess, 20.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "\n",
      "Stage 1 - mAP@0.4: 0.4701\n",
      "Stage 1 - mAP@0.4:0.95: 0.3017\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStage 1 - mAP@0.4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results_stage1.box.map50\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStage 1 - mAP@0.4:0.95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results_stage1.box.map\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStage 1 - Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results_stage1.box.p\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStage 1 - Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results_stage1.box.r\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStage 1 - F1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results_stage1.box.f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "# Run validation on first stage model\n",
    "print(\"\\nValidating first stage model...\")\n",
    "first_stage_model = YOLO('chest_xray/yolov8m_stage1/weights/best.pt')\n",
    "val_results_stage1 = safe_val(\n",
    "    first_stage_model,\n",
    "    data='dataset.yaml',\n",
    "    split='val',\n",
    "    imgsz=1024,\n",
    "    batch=16,\n",
    "    device=0,\n",
    "    verbose=True,\n",
    "    conf=0.25,\n",
    "    iou=0.4,   # PASCAL VOC metric uses IoU > 0.4\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Print first stage metrics\n",
    "if val_results_stage1 is not None and hasattr(val_results_stage1, 'box'):\n",
    "    print(f\"\\nStage 1 - mAP@0.4: {val_results_stage1.box.map50:.4f}\")\n",
    "    print(f\"Stage 1 - mAP@0.4:0.95: {val_results_stage1.box.map:.4f}\")\n",
    "    print(f\"Stage 1 - Precision: {val_results_stage1.box.p:.4f}\")\n",
    "    print(f\"Stage 1 - Recall: {val_results_stage1.box.r:.4f}\")\n",
    "    print(f\"Stage 1 - F1-Score: {val_results_stage1.box.f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3260c50-0c47-43f6-b4da-3e7c99ae6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: TRAINING WITH ALL LAYERS UNFROZEN ---\n",
      "New https://pypi.org/project/ultralytics/8.3.131 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=hyp_medical_stage2.yaml, classes=None, close_mosaic=0, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=[], half=False, hsv_h=0.01, hsv_s=0.1, hsv_v=0.1, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0003, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=chest_xray/yolov8m_stage1/weights/best.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=yolov8m_stage2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=False, profile=False, project=chest_xray, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=chest_xray/yolov8m_stage2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3783802  ultralytics.nn.modules.head.Detect           [14, [192, 384, 576]]         \n",
      "Model summary: 169 layers, 25,864,426 parameters, 25,864,410 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 1867.4Â±441.7 MB/s, size: 443.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/train.cache... 8573 images, 5175 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8573/8573 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1064.2Â±530.8 MB/s, size: 436.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to chest_xray/yolov8m_stage2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0003, momentum=0.937) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 1024 train, 1024 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mchest_xray/yolov8m_stage2\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/25       4.5G      1.529      2.908      1.681          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [05:05<00:00,  7.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.409      0.171      0.173     0.0838\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/25      4.61G      1.463      2.359      1.601          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:59<00:00,  7.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.365      0.175      0.188     0.0913\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/25       4.7G      1.429      2.244      1.569          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.355      0.236      0.219      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/25      4.85G      1.423      2.034      1.554          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.374      0.217      0.218      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/25      4.85G      1.401      1.995      1.542          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.373      0.247      0.231      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/25      4.85G      1.386      1.908      1.526          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.37      0.261      0.244      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/25      4.85G      1.369      1.836      1.516          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.365       0.28      0.255      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/25      4.85G      1.352      1.748      1.497          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.356      0.295      0.266      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/25      4.85G      1.329      1.701      1.482          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.374      0.306      0.275       0.14\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/25      4.85G      1.318      1.644      1.468          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.39       0.32       0.29      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/25      4.85G      1.303      1.583      1.457          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.396      0.328      0.299      0.153\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/25      4.85G      1.285      1.528      1.443          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.405      0.335       0.31      0.157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/25      4.85G      1.265       1.49      1.428          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:23<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.42      0.337      0.319      0.166\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/25      4.85G      1.247      1.438      1.415          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.437      0.336      0.328      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/25      4.85G       1.23       1.41      1.399          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.445      0.343       0.34      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/25      4.85G      1.214      1.367      1.388          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.45      0.359      0.351      0.188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/25      4.85G        1.2      1.326      1.375          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.429      0.361      0.341      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/25      4.85G       1.18      1.291      1.361          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.431       0.35      0.342      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/25      4.85G      1.164      1.258       1.35          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.442       0.36      0.351      0.191\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/25      4.85G      1.148      1.226      1.336          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.46       0.36      0.368      0.202\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/25      4.85G      1.132      1.199      1.327          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.444       0.37      0.365      0.202\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/25      4.85G       1.12      1.175      1.317          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.458      0.367      0.372      0.206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/25      4.85G      1.108      1.158       1.31          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:56<00:00,  7.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.465      0.372      0.378       0.21\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/25      4.85G      1.102      1.146      1.304          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.46      0.373       0.38      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/25      4.85G      1.096      1.137        1.3          0       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2144/2144 [04:57<00:00,  7.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:22<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.47      0.379      0.385      0.216\n",
      "\n",
      "25 epochs completed in 2.656 hours.\n",
      "Optimizer stripped from chest_xray/yolov8m_stage2/weights/last.pt, 52.1MB\n",
      "Optimizer stripped from chest_xray/yolov8m_stage2/weights/best.pt, 52.1MB\n",
      "\n",
      "Validating chest_xray/yolov8m_stage2/weights/best.pt...\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792/792 [01:13<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.469      0.378      0.384      0.215\n",
      "    Aortic enlargement       1749       4053      0.507      0.558      0.587      0.402\n",
      "           Atelectasis        128        198      0.441      0.259      0.277      0.136\n",
      "         Calcification        281        625      0.359      0.323      0.289      0.152\n",
      "          Cardiomegaly       1287       2970      0.748      0.485      0.602      0.429\n",
      "         Consolidation        229        380      0.477      0.329      0.375       0.21\n",
      "                   ILD        265        711      0.572      0.111      0.259      0.142\n",
      "          Infiltration        395        815       0.47      0.337      0.366      0.194\n",
      "          Lung Opacity        836       1606       0.43      0.409      0.377      0.202\n",
      "           Nodule/Mass        534       1740      0.593      0.289      0.374      0.218\n",
      "          Other lesion        728       1510      0.334      0.291      0.252      0.127\n",
      "      Pleural effusion        671       1585      0.376      0.538      0.432      0.203\n",
      "    Pleural thickening       1261       3187      0.358       0.45      0.356      0.162\n",
      "          Pneumothorax         57        143      0.527      0.444      0.426      0.238\n",
      "    Pulmonary fibrosis       1021       2995      0.376      0.472      0.402      0.201\n",
      "Speed: 0.2ms preprocess, 10.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mchest_xray/yolov8m_stage2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Second stage of training - unfreeze all layers\n",
    "print(\"\\n--- STAGE 2: TRAINING WITH ALL LAYERS UNFROZEN ---\")\n",
    "model_stage2 = YOLO('chest_xray/yolov8m_stage1/weights/best.pt')\n",
    "\n",
    "# Modified hyperparameters for second stage\n",
    "hyp_stage2 = hyp.copy()\n",
    "hyp_stage2[\"lr0\"] = 0.0003  # Lower learning rate for fine-tuning\n",
    "hyp_stage2[\"mosaic\"] = 0.0  # Disable mosaic for fine-tuning\n",
    "\n",
    "# Save stage 2 hyperparameters\n",
    "with open('hyp_medical_stage2.yaml', 'w') as f:\n",
    "    yaml.dump(hyp_stage2, f, default_flow_style=False)\n",
    "\n",
    "# Second stage training with all layers unfrozen\n",
    "results_stage2 = model_stage2.train(\n",
    "    data='dataset.yaml',\n",
    "    epochs=25,                  # Continue training\n",
    "    patience=15,                # Early stopping\n",
    "    batch=4,                    # Batch size\n",
    "    imgsz=1024,                 # Image size\n",
    "    device=0,                   # GPU device\n",
    "    val=True,                   # Validate during training\n",
    "    amp=True,                   # Mixed precision training\n",
    "    pretrained=False,           # Don't use pretrained (use our first stage)\n",
    "    cfg='hyp_medical_stage2.yaml', # Stage 2 hyperparameters\n",
    "    optimizer='AdamW',          # Optimizer\n",
    "    project='chest_xray',       # Project name\n",
    "    name='yolov8m_stage2',      # Run name\n",
    "    exist_ok=True,              # Overwrite existing run\n",
    "    cos_lr=True,                # Cosine LR scheduler\n",
    "    close_mosaic=0,             # Disable mosaic\n",
    "    freeze=[],                  # Unfreeze all layers\n",
    "    rect=True,                  # Rectangular training\n",
    "    verbose=True,               # Verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb042b4b-015f-4cac-aeff-28af87ee3f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- COMPREHENSIVE MODEL EVALUATION ---\n",
      "WARNING âš ï¸ 'save_hybrid' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "Model summary (fused): 92 layers, 25,847,866 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1652.3Â±199.2 MB/s, size: 435.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.758      0.173      0.469      0.312\n",
      "    Aortic enlargement       1749       4053      0.801       0.36      0.604      0.453\n",
      "           Atelectasis        128        198      0.762     0.0808      0.427      0.254\n",
      "         Calcification        281        625      0.678      0.128      0.401      0.261\n",
      "          Cardiomegaly       1287       2970      0.895       0.34      0.628      0.493\n",
      "         Consolidation        229        380      0.735     0.0947      0.414      0.285\n",
      "                   ILD        265        711      0.762     0.0225      0.393      0.307\n",
      "          Infiltration        395        815      0.744      0.146      0.445      0.274\n",
      "          Lung Opacity        836       1606      0.707      0.183      0.438      0.282\n",
      "           Nodule/Mass        534       1740      0.801      0.157      0.486      0.349\n",
      "          Other lesion        728       1510      0.677       0.13      0.403      0.243\n",
      "      Pleural effusion        671       1585      0.827      0.178      0.498      0.282\n",
      "    Pleural thickening       1261       3187      0.736      0.163      0.447      0.236\n",
      "          Pneumothorax         57        143       0.75       0.21      0.494      0.372\n",
      "    Pulmonary fibrosis       1021       2995      0.734      0.228      0.483      0.284\n",
      "Speed: 0.3ms preprocess, 20.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Saving runs/detect/val3/predictions.json...\n",
      "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n",
      "\n",
      "Detection Metrics:\n",
      "mAP@0.4: 0.4687\n",
      "mAP@0.4:0.95: 0.3124\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmAP@0.4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results.box.map50\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmAP@0.4:0.95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results.box.map\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results.box.p\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results.box.r\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mF1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_results.box.f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "# Step 13: Comprehensive evaluation with proper metrics\n",
    "print(\"\\n--- COMPREHENSIVE MODEL EVALUATION ---\")\n",
    "\n",
    "# Load the best model from stage 2\n",
    "best_model = YOLO('chest_xray/yolov8m_stage2/weights/best.pt')\n",
    "\n",
    "# Run validation with PASCAL VOC metrics (IoU > 0.4)\n",
    "val_results = safe_val(\n",
    "    best_model,\n",
    "    data='dataset.yaml',\n",
    "    split='val',\n",
    "    imgsz=1024,\n",
    "    batch=16,\n",
    "    device=0,\n",
    "    verbose=True,\n",
    "    conf=0.25,\n",
    "    iou=0.4,   # PASCAL VOC metric uses IoU > 0.4\n",
    "    save_json=True,\n",
    "    save_hybrid=True,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "if val_results is not None and hasattr(val_results, 'box'):\n",
    "    print(\"\\nDetection Metrics:\")\n",
    "    print(f\"mAP@0.4: {val_results.box.map50:.4f}\")\n",
    "    print(f\"mAP@0.4:0.95: {val_results.box.map:.4f}\")\n",
    "    print(f\"Precision: {val_results.box.p:.4f}\")\n",
    "    print(f\"Recall: {val_results.box.r:.4f}\")\n",
    "    print(f\"F1-Score: {val_results.box.f1:.4f}\")\n",
    "else:\n",
    "    print(\"Could not retrieve validation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c5e2a9-5ae4-4a18-8fff-281149b55c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not generate per-class metrics visualization: 'Metric' object has no attribute 'ap_class'. See valid attributes below.\n",
      "\n",
      "    Class for computing evaluation metrics for YOLOv8 model.\n",
      "\n",
      "    Attributes:\n",
      "        p (list): Precision for each class. Shape: (nc,).\n",
      "        r (list): Recall for each class. Shape: (nc,).\n",
      "        f1 (list): F1 score for each class. Shape: (nc,).\n",
      "        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n",
      "        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n",
      "        nc (int): Number of classes.\n",
      "\n",
      "    Methods:\n",
      "        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n",
      "        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n",
      "        mp(): Mean precision of all classes. Returns: Float.\n",
      "        mr(): Mean recall of all classes. Returns: Float.\n",
      "        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n",
      "        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n",
      "        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n",
      "        mean_results(): Mean of results, returns mp, mr, map50, map.\n",
      "        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n",
      "        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n",
      "        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n",
      "        update(results): Update metric attributes with new evaluation results.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Step 14: Per-class metrics\n",
    "try:\n",
    "    class_names = data_yaml['names']\n",
    "    # Extract per-class AP\n",
    "    per_class_ap = {}\n",
    "    per_class_precision = {}\n",
    "    per_class_recall = {}\n",
    "    \n",
    "    if hasattr(val_results, 'box') and hasattr(val_results.box, 'ap_class_index'):\n",
    "        # Extract per-class metrics\n",
    "        for i, idx in enumerate(val_results.box.ap_class_index):\n",
    "            per_class_ap[int(idx)] = float(val_results.box.ap_class[i])\n",
    "    else:\n",
    "        # Alternative way to extract metrics if standard method fails\n",
    "        print(\"Using alternative method to extract per-class metrics...\")\n",
    "        # Try to access metrics directly from validation results\n",
    "        for i in range(len(class_names)):\n",
    "            if hasattr(val_results.box, f'ap{i}'):\n",
    "                per_class_ap[i] = getattr(val_results.box, f'ap{i}')\n",
    "            else:\n",
    "                per_class_ap[i] = 0\n",
    "    \n",
    "    # Create DataFrame for easier visualization\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'AP@0.4': [per_class_ap.get(i, 0) for i in range(len(class_names))],\n",
    "    })\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    metrics_df.to_csv('class_metrics.csv', index=False)\n",
    "    print(\"Per-class metrics saved to class_metrics.csv\")\n",
    "    \n",
    "    # Create a bar plot of AP per class\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bars = plt.bar(metrics_df['Class'], metrics_df['AP@0.4'], color='skyblue')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('AP@0.4')\n",
    "    plt.title('Average Precision by Class (IoU > 0.4)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.01,\n",
    "                 f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ap_by_class.png')\n",
    "    print(\"AP by class visualization saved to ap_by_class.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate per-class metrics visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad176ba-fcad-457e-a0f5-42a7ea3a11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d5cefe2-6923-46ad-8b1c-7f0d8e7b94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINDING OPTIMAL CONFIDENCE THRESHOLD ---\n",
      "Evaluating with confidence threshold: 0.05\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1147.1Â±323.8 MB/s, size: 447.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:16<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.503      0.354      0.431      0.262\n",
      "Speed: 0.4ms preprocess, 20.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val14\u001b[0m\n",
      "Evaluating with confidence threshold: 0.1\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1391.7Â±465.4 MB/s, size: 419.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:15<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.607      0.282      0.448      0.282\n",
      "Speed: 0.3ms preprocess, 20.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val15\u001b[0m\n",
      "Evaluating with confidence threshold: 0.15\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 2291.6Â±1197.7 MB/s, size: 410.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:15<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.684      0.235      0.462      0.298\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val16\u001b[0m\n",
      "Evaluating with confidence threshold: 0.2\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.1 ms, read: 3729.1Â±1915.2 MB/s, size: 408.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:15<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.732      0.201      0.469      0.307\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val17\u001b[0m\n",
      "Evaluating with confidence threshold: 0.25\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3497.4Â±1950.4 MB/s, size: 473.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.758      0.173      0.469      0.312\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val18\u001b[0m\n",
      "Evaluating with confidence threshold: 0.3\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 1927.3Â±690.2 MB/s, size: 429.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.802       0.15      0.479      0.323\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n",
      "Evaluating with confidence threshold: 0.35\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.1 ms, read: 1838.9Â±447.7 MB/s, size: 426.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518       0.84      0.136       0.49      0.336\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val20\u001b[0m\n",
      "Evaluating with confidence threshold: 0.4\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 2917.2Â±2187.9 MB/s, size: 466.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.869      0.118      0.496      0.344\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val21\u001b[0m\n",
      "Evaluating with confidence threshold: 0.5\n",
      "Ultralytics 8.3.130 ðŸš€ Python-3.11.12 torch-2.7.0+cu128 CUDA:0 (NVIDIA RTX 4000 Ada Generation, 20028MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2793.7Â±2063.3 MB/s, size: 387.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/mhedas/common/amunozbr/amia-2025-challenge/yolov8_dataset/labels/val.cache... 6336 images, 3831 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6336/6336 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [02:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6336      22518      0.905     0.0916        0.5      0.363\n",
      "Speed: 0.3ms preprocess, 20.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val22\u001b[0m\n",
      "   Conf     mAP40  mAP40-95  Precision    Recall  F1-Score\n",
      "0  0.05  0.431469  0.261933   0.503102  0.353717  0.406788\n",
      "1  0.10  0.447932  0.281894   0.607260  0.282254  0.378278\n",
      "2  0.15  0.462065  0.297952   0.683954  0.235004  0.340884\n",
      "3  0.20  0.469212  0.307451   0.732028  0.200779  0.306060\n",
      "4  0.25  0.468681  0.312418   0.757715  0.172968  0.272467\n",
      "5  0.30  0.479167  0.322879   0.801628  0.150268  0.242334\n",
      "6  0.35  0.489902  0.336354   0.839533  0.135904  0.222472\n",
      "7  0.40  0.495506  0.344406   0.868596  0.118199  0.195565\n",
      "8  0.50  0.499616  0.362703   0.904936  0.091617  0.153463\n",
      "Optimal confidence threshold based on F1-Score: 0.05\n",
      "Confidence threshold analysis saved to confidence_threshold_metrics.png\n",
      "\n",
      "--- CONFUSION MATRIX ANALYSIS ---\n",
      "Error generating confusion matrix: 'DetectionModel' object has no attribute 'val_loader'\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Find optimal confidence threshold with Pascal VOC metric (IoU > 0.4)\n",
    "print(\"\\n--- FINDING OPTIMAL CONFIDENCE THRESHOLD ---\")\n",
    "conf_thresholds = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
    "threshold_results = []\n",
    "\n",
    "for conf in conf_thresholds:\n",
    "    print(f\"Evaluating with confidence threshold: {conf}\")\n",
    "    \n",
    "    # Run validation with this threshold using PASCAL VOC IoU threshold (0.4)\n",
    "    results = safe_val(\n",
    "        best_model,\n",
    "        data='dataset.yaml',\n",
    "        split='val',\n",
    "        imgsz=1024,\n",
    "        batch=16,\n",
    "        device=0,\n",
    "        verbose=False,\n",
    "        conf=conf,\n",
    "        iou=0.4,  # PASCAL VOC metric uses IoU > 0.4\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    if results is not None and hasattr(results, 'box'):\n",
    "        threshold_results.append({\n",
    "            'Conf': conf,\n",
    "            'mAP40': results.box.map50,  # mAP at IoU=0.4\n",
    "            'mAP40-95': results.box.map,\n",
    "            'Precision': np.mean(results.box.p),\n",
    "            'Recall': np.mean(results.box.r),\n",
    "            'F1-Score': np.mean(results.box.f1)\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Error evaluating confidence threshold {conf}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(threshold_df)\n",
    "\n",
    "# Save results to CSV\n",
    "threshold_df.to_csv('confidence_threshold_results.csv', index=False)\n",
    "\n",
    "# Find optimal threshold based on F1 score\n",
    "if len(threshold_df) > 0:\n",
    "    optimal_f1_idx = threshold_df['F1-Score'].idxmax()\n",
    "    optimal_threshold = threshold_df.loc[optimal_f1_idx, 'Conf']\n",
    "    print(f\"Optimal confidence threshold based on F1-Score: {optimal_threshold}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(threshold_df['Conf'], threshold_df['Precision'], 'b-', label='Precision')\n",
    "    plt.plot(threshold_df['Conf'], threshold_df['Recall'], 'r-', label='Recall')\n",
    "    plt.plot(threshold_df['Conf'], threshold_df['F1-Score'], 'g-', label='F1-Score')\n",
    "    plt.plot(threshold_df['Conf'], threshold_df['mAP40'], 'y-', label='mAP@0.4')\n",
    "    plt.axvline(x=optimal_threshold, color='k', linestyle='--', label=f'Optimal threshold = {optimal_threshold}')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Metrics vs Confidence Threshold (IoU > 0.4)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('confidence_threshold_metrics.png')\n",
    "    print(\"Confidence threshold analysis saved to confidence_threshold_metrics.png\")\n",
    "else:\n",
    "    # Default threshold if evaluation fails\n",
    "    optimal_threshold = 0.25\n",
    "    print(f\"Using default confidence threshold: {optimal_threshold}\")\n",
    "\n",
    "# Step 16: Confusion Matrix Analysis\n",
    "print(\"\\n--- CONFUSION MATRIX ANALYSIS ---\")\n",
    "try:\n",
    "    # Collect predictions and ground truth for confusion matrix\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    val_loader = best_model.val_loader\n",
    "    \n",
    "    for batch_i, (im, targets, paths, shapes) in enumerate(tqdm(val_loader, desc=\"Collecting predictions\")):\n",
    "        # Run inference on batch\n",
    "        results = best_model.predictor(im, conf=optimal_threshold, iou=0.4)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            # Extract predictions\n",
    "            if len(result.boxes) > 0:\n",
    "                pred_classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                for pred_class in pred_classes:\n",
    "                    all_preds.append(pred_class)\n",
    "            else:\n",
    "                # No detection counts as \"No finding\" (class 14)\n",
    "                all_preds.append(14)\n",
    "            \n",
    "            # Extract targets for this image\n",
    "            img_targets = targets[targets[:, 0] == i]\n",
    "            if len(img_targets) > 0:\n",
    "                target_classes = img_targets[:, 1].cpu().numpy().astype(int)\n",
    "                for target_class in target_classes:\n",
    "                    all_targets.append(target_class)\n",
    "            else:\n",
    "                # No target counts as \"No finding\" (class 14)\n",
    "                all_targets.append(14)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds, labels=list(range(15)))\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    class_names_with_nofinding = data_yaml['names'] + ['No finding']\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=class_names_with_nofinding, \n",
    "                yticklabels=class_names_with_nofinding)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    print(\"Confusion matrix saved to confusion_matrix.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating confusion matrix: {e}\")\n",
    "\n",
    "# Step 17: Function for test inference with proper scaling\n",
    "def scale_bbox_back(bbox, png_dims, original_dims):\n",
    "    \"\"\"Scale bounding box coordinates from PNG dimensions back to original dimensions\"\"\"\n",
    "    png_height, png_width = png_dims\n",
    "    orig_height, orig_width = original_dims\n",
    "    \n",
    "    # Scale factors\n",
    "    width_scale = orig_width / png_width\n",
    "    height_scale = orig_height / png_height\n",
    "    \n",
    "    # Scale coordinates back to original dimensions\n",
    "    x_min = int(bbox[0] * width_scale)\n",
    "    y_min = int(bbox[1] * height_scale)\n",
    "    x_max = int(bbox[2] * width_scale)\n",
    "    y_max = int(bbox[3] * height_scale)\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e034d4-bf4e-44e6-bdc5-f5d63a72aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Test Set Inference and Submission\n",
    "print(\"\\n--- TEST SET INFERENCE ---\")\n",
    "print(\"Starting inference on test set...\")\n",
    "\n",
    "# Load test data\n",
    "test_path = os.path.join(root_path, \"test\", \"test\")\n",
    "sample_submission = pd.read_csv(os.path.join(root_path, \"sample_submission.csv\"))\n",
    "test_dimensions = load_image_dimensions(os.path.join(root_path, \"img_size.csv\"))\n",
    "\n",
    "print(f\"Loaded {len(sample_submission)} test images\")\n",
    "\n",
    "# Make predictions on test set using optimal confidence threshold\n",
    "predictions = []\n",
    "test_count = 0\n",
    "no_detection_count = 0\n",
    "\n",
    "for img_id in tqdm(sample_submission['image_id'], desc=\"Running inference on test set\"):\n",
    "    img_path = os.path.join(test_path, f\"{img_id}.png\")\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        test_count += 1\n",
    "        # Get original dimensions for this image\n",
    "        orig_dims = test_dimensions[img_id]  \n",
    "        \n",
    "        # Run inference with optimal confidence threshold and PASCAL VOC IoU (0.4)\n",
    "        results = best_model(img_path, conf=optimal_threshold, iou=0.4)\n",
    "        \n",
    "        # Get detections\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()  # in 1024x1024 scale\n",
    "        scores = results[0].boxes.conf.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            # No findings detected - use class 14 (\"No finding\")\n",
    "            no_detection_count += 1\n",
    "            predictions.append(f\"{img_id},14 1.0 0 0 1 1\")\n",
    "        else:\n",
    "            # Scale boxes back to original dimensions\n",
    "            img_preds = []\n",
    "            for box, score, cls in zip(boxes, scores, classes):\n",
    "                # Scale box back to original dimensions\n",
    "                scaled_box = scale_bbox_back(box, (1024, 1024), orig_dims)\n",
    "                img_preds.append(f\"{cls} {score:.6f} {scaled_box[0]} {scaled_box[1]} {scaled_box[2]} {scaled_box[3]}\")\n",
    "            \n",
    "            predictions.append(f\"{img_id},{' '.join(img_preds)}\")\n",
    "    else:\n",
    "        print(f\"Test image not found: {img_path}\")\n",
    "\n",
    "print(f\"Processed {test_count} test images\")\n",
    "print(f\"Number of images with no detections (classified as 'No finding'): {no_detection_count}\")\n",
    "print(f\"Percentage of 'No finding' classifications: {(no_detection_count/test_count)*100:.2f}%\")\n",
    "\n",
    "# Create submission file\n",
    "submission_path = 'submission.csv'\n",
    "with open(submission_path, 'w') as f:\n",
    "    f.write(\"image_id,PredictionString\\n\")\n",
    "    for pred in predictions:\n",
    "        img_id, target = pred.split(',', 1)\n",
    "        f.write(f\"{img_id},{target}\\n\")\n",
    "\n",
    "print(f\"Submission file created at {submission_path}\")\n",
    "\n",
    "# Step 19: Visualize some test predictions\n",
    "print(\"\\n--- CREATING TEST VISUALIZATIONS ---\")\n",
    "\n",
    "try:\n",
    "    # Sample a few test images\n",
    "    sample_size = min(5, test_count)\n",
    "    sample_ids = np.random.choice(sample_submission['image_id'].values, size=sample_size, replace=False)\n",
    "    \n",
    "    print(f\"Generating visualizations for {sample_size} random test images...\")\n",
    "    \n",
    "    for img_id in sample_ids:\n",
    "        img_path = os.path.join(test_path, f\"{img_id}.png\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Run inference with optimal threshold\n",
    "        results = best_model(img_path, conf=optimal_threshold, iou=0.4)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(img_array, cmap='gray')\n",
    "        plt.title(f\"Test Image: {img_id}\")\n",
    "        \n",
    "        # Plot predictions\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        scores = results[0].boxes.conf.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            plt.text(10, 30, \"Prediction: No finding\", fontsize=14, \n",
    "                     color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        else:\n",
    "            for box, score, cls in zip(boxes, scores, classes):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                \n",
    "                rect = plt.Rectangle((x_min, y_min), width, height, \n",
    "                                    fill=False, edgecolor='red', linewidth=2)\n",
    "                plt.gca().add_patch(rect)\n",
    "                \n",
    "                class_name = data_yaml['names'][cls] if cls < len(data_yaml['names']) else f\"Class {cls}\"\n",
    "                plt.text(x_min, y_min - 5, f\"{class_name}: {score:.2f}\", \n",
    "                         color='white', fontsize=10, \n",
    "                         bbox=dict(facecolor='red', alpha=0.5))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"test_prediction_{img_id}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Test visualizations saved as test_prediction_*.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate test visualizations: {e}\")\n",
    "\n",
    "print(\"\\n--- TRAINING AND EVALUATION COMPLETE ---\")\n",
    "print(f\"Best model saved at: chest_xray/yolov8m_stage2/weights/best.pt\")\n",
    "print(f\"Optimal confidence threshold: {optimal_threshold}\")\n",
    "print(f\"Submission file: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5593cea-0e48-42f1-9fcf-17b0869cb8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created at submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission_path = 'submission.csv'\n",
    "with open(submission_path, 'w') as f:\n",
    "    f.write(\"image_id,PredictionString\\n\")\n",
    "    for pred in predictions:\n",
    "        img_id, target = pred.split(',', 1)\n",
    "        f.write(f\"{img_id},{target}\\n\")\n",
    "\n",
    "print(f\"Submission file created at {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 FINAL",
   "language": "python",
   "name": "amia-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
